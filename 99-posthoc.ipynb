{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### _Dataset Creation Sensitivity Analysis - Label Noise_"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Packages\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Data\n",
        "df = pd.read_csv('data.csv')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "RANDOM_SEED = 42\n",
        "NOISE_LEVELS = [0.5, 1.0, 1.5]\n",
        "MIN_BRIX = 2\n",
        "MAX_BRIX = None "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "rng = np.random.default_rng(RANDOM_SEED)\n",
        "\n",
        "# Define corresponding DataFrame names and file name suffixes\n",
        "df_names = ['df_05noise', 'df_10noise', 'df_15noise']\n",
        "file_suffixes = ['0_5', '1_0', '1_5']\n",
        "dfs = {}\n",
        "\n",
        "# Generate DataFrames with noisy and clipped 'Brix (Position)'\n",
        "for sigma, name, suffix in zip(NOISE_LEVELS, df_names, file_suffixes):\n",
        "    df_copy = df.copy()\n",
        "    noise = rng.normal(loc=0.0, scale=sigma, size=len(df_copy))\n",
        "    noisy_brix = df_copy['Brix (Position)'] + noise\n",
        "    df_copy['Brix (Position)'] = np.clip(noisy_brix, MIN_BRIX, MAX_BRIX)\n",
        "    dfs[name] = df_copy\n",
        "\n",
        "    # Save to CSV\n",
        "    df_copy.to_csv(f\"/data/data_sensitivityanalysis_noise_{suffix}.csv\", index=False)\n",
        "\n",
        "# Unpack the individual DataFrames\n",
        "df_05noise = dfs['df_05noise']\n",
        "df_10noise = dfs['df_10noise']\n",
        "df_15noise = dfs['df_15noise']"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### _Dataset Creation Sensitivity Analysis - Training Set Size_"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Packages\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Data\n",
        "df = pd.read_csv('data.csv')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "RANDOM_SEED = 42"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter non-2025 and non-TestVariety rows\n",
        "df_filtered = df[\n",
        "    (df['Scan Date Year'] != 2025) &\n",
        "    (df['Variety'] != 'TestVariety')\n",
        "]\n",
        "\n",
        "# Always-include rows (2025 and/or TestVariety)\n",
        "df_always = df[\n",
        "    (df['Scan Date Year'] == 2025) |\n",
        "    (df['Variety'] == 'TestVariety')\n",
        "]\n",
        "\n",
        "# Define fractions and output filenames\n",
        "fractions = [0.8, 0.6]\n",
        "output_suffixes = ['_80', '_60']\n",
        "\n",
        "for frac, suffix in zip(fractions, output_suffixes):\n",
        "    df_sampled, _ = train_test_split(\n",
        "        df_filtered,\n",
        "        train_size=frac,\n",
        "        stratify=df_filtered['Variety'],\n",
        "        random_state=RANDOM_SEED\n",
        "    )\n",
        "\n",
        "    df_subset = pd.concat([df_always, df_sampled], ignore_index=True)\n",
        "\n",
        "    df_subset.to_csv(\n",
        "        f'../../data/data_sensitivityanalysis_data_size{suffix}.csv',\n",
        "        index=False\n",
        "    )\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### _Model Comparison - MCS Procedure_"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Packages\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from sklearn.utils import resample\n",
        "from arch.bootstrap import MCS"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Results paths\n",
        "\n",
        "# Path in which all csv predictions and observations are stored\n",
        "base_path = \"../run/run_results\"\n",
        "\n",
        "# File names for OOC results\n",
        "model_files_variety = {\n",
        "    \"PLS\": \"pls_variety.csv\",\n",
        "    \"irPLS\": \"irpls_variety.csv\",\n",
        "    \"lwPLS\": \"lwpls_variety.csv\",\n",
        "    \"CNN\": \"cnn_variety.csv\",\n",
        "    \"BayesConv-CNN\": \"bayesconvcnn_variety.csv\",\n",
        "    \"BayesFC-CNN\": \"bayesfccnn_variety.csv\",\n",
        "    \"SpecTransformer\": \"spectransformer_variety.csv\"\n",
        "}\n",
        "\n",
        "# File names for OOS results\n",
        "model_files_season = {\n",
        "    \"PLS\": \"pls_season.csv\",\n",
        "    \"irPLS\": \"irpls_season.csv\",\n",
        "    \"lwPLS\": \"lwpls_season.csv\",\n",
        "    \"CNN\": \"cnn_season.csv\",\n",
        "    \"BayesConv-CNN\": \"bayesconvcnn_season.csv\",\n",
        "    \"BayesFC-CNN\": \"bayesfccnn_season.csv\",\n",
        "    \"SpecTransformer\": \"spectransformer_season.csv\"\n",
        "}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions\n",
        "def load_predictions(file_dict, base_path):\n",
        "    \"\"\"\n",
        "    Load prediction files and ensure consistent ground truth values.\n",
        "\n",
        "    Parameters:\n",
        "        file_dict : dict\n",
        "            Mapping of model names to CSV filenames.\n",
        "        base_path : str\n",
        "            Directory containing the prediction CSVs.\n",
        "\n",
        "    Returns:\n",
        "        y_true_ref : np.ndarray\n",
        "            Ground truth values (shared across all models).\n",
        "        preds_dict : dict\n",
        "            Dictionary mapping model names to their predicted values.\n",
        "    \"\"\"\n",
        "    preds_dict = {}\n",
        "    y_true_ref = None  # Initialize ground truth reference\n",
        "\n",
        "    for label, filename in file_dict.items():\n",
        "        path = os.path.join(base_path, filename)\n",
        "        df = pd.read_csv(path)\n",
        "\n",
        "        # Standardize column names\n",
        "        df.columns = [col.strip().lower() for col in df.columns]\n",
        "\n",
        "        # Detect column names and extract true and predicted values\n",
        "        if \"true\" in df.columns and \"predicted\" in df.columns:\n",
        "            y_true = df[\"true\"].values\n",
        "            y_pred = df[\"predicted\"].values\n",
        "        elif \"observed\" in df.columns and \"predicted\" in df.columns:\n",
        "            y_true = df[\"observed\"].values\n",
        "            y_pred = df[\"predicted\"].values\n",
        "        else:\n",
        "            raise ValueError(f\"{filename} must contain 'True' or 'Observed' with 'Predicted'.\")\n",
        "\n",
        "        # Validate that all models share the same ground truth\n",
        "        if y_true_ref is None:\n",
        "            y_true_ref = y_true\n",
        "        elif not np.allclose(y_true, y_true_ref):\n",
        "            raise ValueError(f\"{filename} has mismatched true values.\")\n",
        "\n",
        "        # Store model predictions\n",
        "        preds_dict[label] = y_pred\n",
        "\n",
        "    return y_true_ref, preds_dict\n",
        "\n",
        "def bootstrap_r2_matrix(y_true, preds_dict, n_bootstrap=5000):\n",
        "    \"\"\"\n",
        "    Compute a bootstrap distribution of R² scores for each model.\n",
        "\n",
        "    Parameters:\n",
        "        y_true : np.ndarray\n",
        "            Ground truth values.\n",
        "        preds_dict : dict\n",
        "            Dictionary of model predictions.\n",
        "        n_bootstrap : int\n",
        "            Number of bootstrap resamples.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame : DataFrame with model names as columns and R² scores as rows.\n",
        "    \"\"\"\n",
        "    model_names = list(preds_dict.keys())\n",
        "    r2_matrix = {name: [] for name in model_names}\n",
        "    y_true = np.array(y_true)\n",
        "\n",
        "    for _ in range(n_bootstrap):\n",
        "        idx = resample(np.arange(len(y_true)), replace=True)\n",
        "        for name in model_names:\n",
        "            r2 = r2_score(y_true[idx], preds_dict[name][idx])\n",
        "            r2_matrix[name].append(r2)\n",
        "\n",
        "    return pd.DataFrame(r2_matrix)\n",
        "\n",
        "def get_r2_mcs(y_true, preds_dict, alpha=0.05, n_bootstrap=5000):\n",
        "    \"\"\"\n",
        "    Perform Model Confidence Set (MCS) analysis using R² as the evaluation metric.\n",
        "\n",
        "    Parameters:\n",
        "        y_true : np.ndarray\n",
        "            Ground truth values.\n",
        "        preds_dict : dict\n",
        "            Dictionary of model predictions.\n",
        "        alpha : float\n",
        "            Significance level.\n",
        "        n_bootstrap : int\n",
        "            Number of bootstrap resamples.\n",
        "\n",
        "    Returns:\n",
        "        tuple : (models in MCS, best model, R² bootstrap DataFrame)\n",
        "    \"\"\"\n",
        "    r2_df = bootstrap_r2_matrix(y_true, preds_dict, n_bootstrap=n_bootstrap)\n",
        "    best_model = r2_df.mean().idxmax()  # Identify model with highest average R²\n",
        "    mcs_models = []\n",
        "\n",
        "    for model in r2_df.columns:\n",
        "        if model == best_model:\n",
        "            mcs_models.append(model)\n",
        "            continue\n",
        "\n",
        "        # Confidence interval of R² difference (best - other)\n",
        "        diff = r2_df[best_model] - r2_df[model]\n",
        "        lower, upper = np.percentile(diff, [100 * alpha / 2, 100 * (1 - alpha / 2)])\n",
        "\n",
        "        # Include model if its R² is not significantly worse than the best\n",
        "        if lower <= 0 <= upper:\n",
        "            mcs_models.append(model)\n",
        "\n",
        "    return mcs_models, best_model, r2_df\n",
        "\n",
        "def get_loss_matrix(y_true, preds_dict, metric):\n",
        "    \"\"\"\n",
        "    Compute the loss matrix based on the chosen evaluation metric.\n",
        "\n",
        "    Parameters:\n",
        "        y_true : np.ndarray\n",
        "            Ground truth values.\n",
        "        preds_dict : dict\n",
        "            Dictionary of model predictions.\n",
        "        metric : str\n",
        "            Evaluation metric: 'rmsep', 'practical_accuracy', or 'r2'.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame : Loss matrix (None if metric is 'r2').\n",
        "    \"\"\"\n",
        "    losses = {}\n",
        "    y_true = np.array(y_true)\n",
        "\n",
        "    if metric == \"rmsep\":\n",
        "        for name, preds in preds_dict.items():\n",
        "            losses[name] = (y_true - preds) ** 2  # Squared error per sample\n",
        "        return pd.DataFrame(losses)\n",
        "\n",
        "    elif metric == \"practical_accuracy\":\n",
        "        for name, preds in preds_dict.items():\n",
        "            # Binary accuracy within ±20% range, then convert to loss\n",
        "            acc = (np.abs(y_true - preds) <= 0.2 * np.abs(y_true)).astype(int)\n",
        "            losses[name] = 1 - acc\n",
        "        return pd.DataFrame(losses)\n",
        "\n",
        "    elif metric == \"r2\":\n",
        "        return None  # Handled separately using bootstrap R²\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Metric must be one of: 'r2', 'rmsep', 'practical_accuracy'\")\n",
        "\n",
        "def run_mcs(\n",
        "    y_true,\n",
        "    preds_dict,\n",
        "    metric,\n",
        "    alpha=0.05,\n",
        "    reps=5000\n",
        "):\n",
        "    \"\"\"\n",
        "    Perform Model Confidence Set (MCS) analysis using specified performance metric.\n",
        "\n",
        "    Parameters:\n",
        "        y_true : np.ndarray\n",
        "            Ground truth values.\n",
        "        preds_dict : dict\n",
        "            Dictionary of model predictions.\n",
        "        metric : str\n",
        "            Evaluation metric: 'rmsep', 'practical_accuracy', or 'r2'.\n",
        "        alpha : float\n",
        "            Significance level.\n",
        "        reps : int\n",
        "            Number of bootstrap or permutation repetitions.\n",
        "\n",
        "    Returns:\n",
        "        list : Model names included in the MCS.\n",
        "    \"\"\"\n",
        "    if metric == \"r2\":\n",
        "        models, best, _ = get_r2_mcs(y_true, preds_dict, alpha=alpha, n_bootstrap=reps)\n",
        "        return models\n",
        "    else:\n",
        "        # Compute sample-level losses and run external MCS test\n",
        "        loss_matrix = get_loss_matrix(y_true, preds_dict, metric)\n",
        "        mcs = MCS(loss_matrix.values, size=alpha, reps=reps)\n",
        "        mcs.compute()\n",
        "        return loss_matrix.columns[mcs.included].tolist()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Run\n",
        "results = {}\n",
        "\n",
        "# Loop over both dataset splits: 'Variety' and 'Season'\n",
        "for label, model_files in {\"Variety\": model_files_variety, \"Season\": model_files_season}.items():\n",
        "    \n",
        "    # Load ground truth and prediction dictionaries\n",
        "    y_true, preds_dict = load_predictions(model_files, base_path)\n",
        "    results[label] = {}\n",
        "\n",
        "    # Run MCS for each metric\n",
        "    for metric in [\"rmsep\", \"r2\", \"practical_accuracy\"]:\n",
        "        print(f\"\\nRunning MCS for {label} - {metric.upper()}\")\n",
        "\n",
        "        # Compute Model Confidence Set (MCS)\n",
        "        mcs_models = run_mcs(y_true, preds_dict, metric, alpha=0.05, reps=5000)\n",
        "        results[label][metric] = mcs_models\n",
        "\n",
        "        print(f\"Models in 95% MCS ({label}, {metric}): {mcs_models}\")\n",
        "\n",
        "# Print summary\n",
        "print(\"\\nSummary of MCS Results:\")\n",
        "for dataset in results:\n",
        "    for metric in results[dataset]:\n",
        "        print(f\"{dataset} - {metric.upper()}: {results[dataset][metric]}\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### _Uncertainty Estimation - 100 MC Forward Passes_"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "# === Load model predictions and uncertainties ===\n",
        "base_path = \"../run/run_results\"\n",
        "\n",
        "# Load BayesConv-CNN and BayesFC-CNN results\n",
        "df_bayesconv_season  = pd.read_csv(os.path.join(base_path, \"bayesconvcnn_season.csv\"))\n",
        "df_bayesconv_variety = pd.read_csv(os.path.join(base_path, \"bayesconvcnn_variety.csv\"))\n",
        "df_bayesfc_season    = pd.read_csv(os.path.join(base_path, \"bayesfccnn_season.csv\"))\n",
        "df_bayesfc_variety   = pd.read_csv(os.path.join(base_path, \"bayesfccnn_variety.csv\"))\n",
        "\n",
        "# Load all model predictions for variety split\n",
        "model_dfs_variety = {\n",
        "    \"pls\": pd.read_csv(os.path.join(base_path, \"pls_variety.csv\")),\n",
        "    \"irpls\": pd.read_csv(os.path.join(base_path, \"irpls_variety.csv\")),\n",
        "    \"lwpls\": pd.read_csv(os.path.join(base_path, \"lwpls_variety.csv\")),\n",
        "    \"cnn\": pd.read_csv(os.path.join(base_path, \"cnn_variety.csv\")),\n",
        "    \"bayesconv\": df_bayesconv_variety,\n",
        "    \"bayesfc\": df_bayesfc_variety,\n",
        "    \"spectransformer\": pd.read_csv(os.path.join(base_path, \"spectransformer_variety.csv\"))\n",
        "}\n",
        "\n",
        "# Load all model predictions for season split\n",
        "model_dfs_season = {\n",
        "    \"pls\": pd.read_csv(os.path.join(base_path, \"pls_season.csv\")),\n",
        "    \"irpls\": pd.read_csv(os.path.join(base_path, \"irpls_season.csv\")),\n",
        "    \"lwpls\": pd.read_csv(os.path.join(base_path, \"lwpls_season.csv\")),\n",
        "    \"cnn\": pd.read_csv(os.path.join(base_path, \"cnn_season.csv\")),\n",
        "    \"bayesconv\": df_bayesconv_season,\n",
        "    \"bayesfc\": df_bayesfc_season,\n",
        "    \"spectransformer\": pd.read_csv(os.path.join(base_path, \"spectransformer_season.csv\"))\n",
        "}\n",
        "\n",
        "# Human-readable labels for plotting\n",
        "label_map = {\n",
        "    \"pls\": \"PLS\",\n",
        "    \"irpls\": \"irPLS\",\n",
        "    \"lwpls\": \"lwPLS\",\n",
        "    \"cnn\": \"CNN\",\n",
        "    \"bayesconv\": \"BayesConv-CNN\",\n",
        "    \"bayesfc\": \"BayesFC-CNN\",\n",
        "    \"spectransformer\": \"SpecTransformer\"\n",
        "}\n",
        "\n",
        "# List of models to include in plots\n",
        "models_to_plot = list(label_map.keys())\n",
        "\n",
        "# === Spearman correlation test between uncertainty and performance metric ===\n",
        "def test_spearman_correlation_between_uncertainty_and_metric(\n",
        "    metric_results,\n",
        "    fractions,\n",
        "    metric_name\n",
        "):\n",
        "    print(f\"\\n=== Spearman Correlation: Uncertainty vs. {metric_name.upper()} ===\")\n",
        "    for model, values in metric_results.items():\n",
        "        r, p = spearmanr(fractions, values)\n",
        "        direction = \"increasing\" if r > 0 else \"decreasing\"\n",
        "        significance = \"significant\" if p < 0.05 else \"not significant\"\n",
        "        print(f\"{model:<15} | Spearman r = {r:>6.3f} | p = {p:>7.4f} | {direction}, {significance}\")\n",
        "\n",
        "# === Evaluate model performance over uncertain subsets and plot ===\n",
        "def evaluate_and_plot_all_models_by_uncertainty(\n",
        "    df_uncertainty,\n",
        "    model_dfs,\n",
        "    model_names,\n",
        "    model_label_map,\n",
        "    test_type,\n",
        "    backbone_model_name\n",
        "):\n",
        "    # Preprocess uncertainty DataFrame and compute standard deviation across MC passes\n",
        "    df_uncertainty.columns = [c.lower().strip() for c in df_uncertainty.columns]\n",
        "    mc_cols = [c for c in df_uncertainty.columns if c.startswith(\"mc_pass_\")]\n",
        "    mc_preds = df_uncertainty[mc_cols].values\n",
        "    mc_std = np.std(mc_preds, axis=1)  # Uncertainty per sample\n",
        "    sorted_indices = np.argsort(-mc_std)  # Sort by decreasing uncertainty\n",
        "\n",
        "    # Load ground truth values from reference model\n",
        "    df_ref = model_dfs[model_names[0]].copy()\n",
        "    df_ref.columns = [c.lower().strip() for c in df_ref.columns]\n",
        "    y_true = df_ref[\"true\"].values if \"true\" in df_ref.columns else df_ref[\"observed\"].values\n",
        "\n",
        "    # Initialize evaluation results for each model\n",
        "    fractions = np.linspace(0.01, 1.0, 100)\n",
        "    rmsep_results = {model: [] for model in model_names}\n",
        "    acc_results = {model: [] for model in model_names}\n",
        "\n",
        "    # Loop over increasing fractions of most uncertain samples\n",
        "    for frac in fractions:\n",
        "        k = int(len(sorted_indices) * frac)\n",
        "        subset = sorted_indices[:k]\n",
        "\n",
        "        # Evaluate each model on selected subset\n",
        "        for model in model_names:\n",
        "            df = model_dfs[model].copy()\n",
        "            df.columns = [c.lower().strip() for c in df.columns]\n",
        "            y = df[\"true\"].values if \"true\" in df.columns else df[\"observed\"].values\n",
        "            y_pred = df[\"predicted\"].values\n",
        "\n",
        "            y_sub = y[subset]\n",
        "            p_sub = y_pred[subset]\n",
        "\n",
        "            rmsep = np.sqrt(mean_squared_error(y_sub, p_sub))\n",
        "            acc = np.mean(np.abs(p_sub - y_sub) <= 0.2 * y_sub)\n",
        "\n",
        "            rmsep_results[model].append(rmsep)\n",
        "            acc_results[model].append(acc)\n",
        "\n",
        "    # === Plot RMSEP over uncertainty fractions ===\n",
        "    print(f\"\\n--> Plotting RMSEP for {backbone_model_name} ({test_type})\")\n",
        "    plt.figure(figsize=(5.5, 3))\n",
        "    for model in model_names:\n",
        "        plt.plot(fractions * 100, rmsep_results[model], label=model_label_map[model])\n",
        "    plt.xlabel(\"Top % Most Uncertain Samples\", fontsize=9)\n",
        "    plt.ylabel(\"RMSEP\", fontsize=9)\n",
        "    plt.xticks(fontsize=8)\n",
        "    plt.yticks(fontsize=8)\n",
        "    plt.legend(fontsize=8, frameon=False)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Correlation between uncertainty and RMSEP\n",
        "    test_spearman_correlation_between_uncertainty_and_metric(\n",
        "        metric_results=rmsep_results,\n",
        "        fractions=fractions,\n",
        "        metric_name=\"rmsep\"\n",
        "    )\n",
        "\n",
        "    # === Plot Practical Accuracy over uncertainty fractions ===\n",
        "    print(f\"\\n--> Plotting Practical Accuracy for {backbone_model_name} ({test_type})\")\n",
        "    plt.figure(figsize=(5.5, 3))\n",
        "    for model in model_names:\n",
        "        plt.plot(fractions * 100, acc_results[model], label=model_label_map[model])\n",
        "    plt.xlabel(\"Top % Most Uncertain Samples\", fontsize=9)\n",
        "    plt.ylabel(\"Practical Accuracy\", fontsize=9)\n",
        "    plt.xticks(fontsize=8)\n",
        "    plt.yticks(fontsize=8)\n",
        "    plt.legend(fontsize=8, frameon=False)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Correlation between uncertainty and accuracy\n",
        "    test_spearman_correlation_between_uncertainty_and_metric(\n",
        "        metric_results=acc_results,\n",
        "        fractions=fractions,\n",
        "        metric_name=\"accuracy\"\n",
        "    )\n",
        "\n",
        "# === Run evaluation for variety split ===\n",
        "evaluate_and_plot_all_models_by_uncertainty(\n",
        "    df_uncertainty=df_bayesfc_variety,\n",
        "    model_dfs=model_dfs_variety,\n",
        "    model_names=models_to_plot,\n",
        "    model_label_map=label_map,\n",
        "    test_type=\"variety\",\n",
        "    backbone_model_name=\"BayesFC-CNN\"\n",
        ")\n",
        "\n",
        "# === Run evaluation for season split ===\n",
        "evaluate_and_plot_all_models_by_uncertainty(\n",
        "    df_uncertainty=df_bayesfc_season,\n",
        "    model_dfs=model_dfs_season,\n",
        "    model_names=models_to_plot,\n",
        "    model_label_map=label_map,\n",
        "    test_type=\"season\",\n",
        "    backbone_model_name=\"BayesFC-CNN\"\n",
        ")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### _Uncertainty Estimation - 10 MC Forward Passes_"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "# === Load model predictions and BayesFC-CNN uncertainties ===\n",
        "base_path = \"../run/run_results\"\n",
        "\n",
        "# Load prediction files with Monte Carlo dropout results\n",
        "df_bayesfc_variety = pd.read_csv(os.path.join(base_path, \"bayesfccnn_variety.csv\"))\n",
        "df_bayesfc_season  = pd.read_csv(os.path.join(base_path, \"bayesfccnn_season.csv\"))\n",
        "\n",
        "# Load all model predictions (variety split)\n",
        "model_dfs_variety = {\n",
        "    \"pls\": pd.read_csv(os.path.join(base_path, \"pls_variety.csv\")),\n",
        "    \"irpls\": pd.read_csv(os.path.join(base_path, \"irpls_variety.csv\")),\n",
        "    \"lwpls\": pd.read_csv(os.path.join(base_path, \"lwpls_variety.csv\")),\n",
        "    \"cnn\": pd.read_csv(os.path.join(base_path, \"cnn_variety.csv\")),\n",
        "    \"bayesconv\": pd.read_csv(os.path.join(base_path, \"bayesconvcnn_variety.csv\")),\n",
        "    \"bayesfc\": df_bayesfc_variety,\n",
        "    \"spectransformer\": pd.read_csv(os.path.join(base_path, \"spectransformer_variety.csv\"))\n",
        "}\n",
        "\n",
        "# Load all model predictions (season split)\n",
        "model_dfs_season = {\n",
        "    \"pls\": pd.read_csv(os.path.join(base_path, \"pls_season.csv\")),\n",
        "    \"irpls\": pd.read_csv(os.path.join(base_path, \"irpls_season.csv\")),\n",
        "    \"lwpls\": pd.read_csv(os.path.join(base_path, \"lwpls_season.csv\")),\n",
        "    \"cnn\": pd.read_csv(os.path.join(base_path, \"cnn_season.csv\")),\n",
        "    \"bayesconv\": pd.read_csv(os.path.join(base_path, \"bayesconvcnn_season.csv\")),\n",
        "    \"bayesfc\": df_bayesfc_season,\n",
        "    \"spectransformer\": pd.read_csv(os.path.join(base_path, \"spectransformer_season.csv\"))\n",
        "}\n",
        "\n",
        "# Human-readable model labels\n",
        "label_map = {\n",
        "    \"pls\": \"PLS\",\n",
        "    \"irpls\": \"irPLS\",\n",
        "    \"lwpls\": \"lwPLS\",\n",
        "    \"cnn\": \"CNN\",\n",
        "    \"bayesconv\": \"BayesConv-CNN\",\n",
        "    \"bayesfc\": \"BayesFC-CNN\",\n",
        "    \"spectransformer\": \"SpecTransformer\"\n",
        "}\n",
        "\n",
        "# Models to include in analysis\n",
        "models_to_plot = list(label_map.keys())\n",
        "\n",
        "# === Bootstrapped Evaluation Function ===\n",
        "def run_bootstrapped_uncertainty_eval(\n",
        "    df_uncertainty,\n",
        "    model_dfs,\n",
        "    model_names,\n",
        "    metric: str,\n",
        "    n_mc_passes: int = 10,\n",
        "    n_bootstrap: int = 5000\n",
        "):\n",
        "    # Preprocess column names and extract MC predictions\n",
        "    df_uncertainty.columns = [c.lower().strip() for c in df_uncertainty.columns]\n",
        "    mc_cols_all = [c for c in df_uncertainty.columns if c.startswith(\"mc_pass_\")]\n",
        "    mc_preds_all = df_uncertainty[mc_cols_all].values\n",
        "\n",
        "    n_samples = mc_preds_all.shape[0]\n",
        "    fractions = np.linspace(0.01, 1.0, 100)\n",
        "    metric_curves = {model: np.zeros((n_bootstrap, len(fractions))) for model in model_names}\n",
        "\n",
        "    # Load ground truth values from reference model\n",
        "    df_ref = model_dfs[model_names[0]].copy()\n",
        "    df_ref.columns = [c.lower().strip() for c in df_ref.columns]\n",
        "    y_true = df_ref[\"true\"].values if \"true\" in df_ref.columns else df_ref[\"observed\"].values\n",
        "\n",
        "    # Perform bootstrapping\n",
        "    for b in range(n_bootstrap):\n",
        "        # Randomly sample MC passes\n",
        "        sampled_cols = np.random.choice(mc_preds_all.shape[1], size=n_mc_passes, replace=False)\n",
        "        mc_sample = mc_preds_all[:, sampled_cols]\n",
        "\n",
        "        # Compute uncertainty and sort indices\n",
        "        uncertainty_std = np.std(mc_sample, axis=1)\n",
        "        sorted_indices = np.argsort(-uncertainty_std)\n",
        "\n",
        "        for i, frac in enumerate(fractions):\n",
        "            k = int(n_samples * frac)\n",
        "            subset = sorted_indices[:k]\n",
        "\n",
        "            for model in model_names:\n",
        "                df = model_dfs[model].copy()\n",
        "                df.columns = [c.lower().strip() for c in df.columns]\n",
        "                y = df[\"true\"].values if \"true\" in df.columns else df[\"observed\"].values\n",
        "                y_pred = df[\"predicted\"].values\n",
        "\n",
        "                y_sub = y[subset]\n",
        "                p_sub = y_pred[subset]\n",
        "\n",
        "                # Compute metric (RMSEP or practical accuracy)\n",
        "                if metric == \"rmsep\":\n",
        "                    value = np.sqrt(mean_squared_error(y_sub, p_sub))\n",
        "                elif metric == \"accuracy\":\n",
        "                    value = np.mean(np.abs(p_sub - y_sub) <= 0.2 * y_sub)\n",
        "                else:\n",
        "                    raise ValueError(\"Metric must be 'rmsep' or 'accuracy'.\")\n",
        "\n",
        "                metric_curves[model][b, i] = value\n",
        "\n",
        "    return metric_curves, fractions\n",
        "\n",
        "# === Spearman correlation evaluation ===\n",
        "def compute_bootstrap_spearman_correlations(\n",
        "    metric_curves: dict,\n",
        "    fractions: np.ndarray,\n",
        "    metric_name: str\n",
        "):\n",
        "    print(f\"\\n=== Bootstrap Spearman Correlation: Uncertainty vs. {metric_name.upper()} ===\")\n",
        "    for model, runs in metric_curves.items():\n",
        "        spearman_rs = []\n",
        "        p_values = []\n",
        "\n",
        "        # Compute Spearman correlation per bootstrap run\n",
        "        for run in runs:\n",
        "            r, p = spearmanr(fractions, run)\n",
        "            spearman_rs.append(r)\n",
        "            p_values.append(p)\n",
        "\n",
        "        # Summary statistics\n",
        "        spearman_rs = np.array(spearman_rs)\n",
        "        p_values = np.array(p_values)\n",
        "        r_mean = spearman_rs.mean()\n",
        "        r_std = spearman_rs.std()\n",
        "        p_05 = np.mean(p_values < 0.05)\n",
        "        p_01 = np.mean(p_values < 0.01)\n",
        "        p_001 = np.mean(p_values < 0.001)\n",
        "\n",
        "        print(f\"{model:<15} | r = {r_mean:>6.3f} ± {r_std:>5.3f} | p < .05: {p_05:.1%} | p < .01: {p_01:.1%} | p < .001: {p_001:.1%}\")\n",
        "\n",
        "# === Plotting function ===\n",
        "def plot_bootstrap_metric(\n",
        "    metric_curves,\n",
        "    fractions,\n",
        "    title,\n",
        "    ylabel\n",
        "):\n",
        "    print(f\"--> Plotting: {title}\")\n",
        "    plt.figure(figsize=(5.5, 3))\n",
        "    for model in models_to_plot:\n",
        "        mean_vals = metric_curves[model].mean(axis=0)\n",
        "        plt.plot(fractions * 100, mean_vals, label=label_map[model])\n",
        "    plt.xlabel(\"Top % Most Uncertain Samples\", fontsize=9)\n",
        "    plt.ylabel(ylabel, fontsize=9)\n",
        "    plt.xticks(fontsize=8)\n",
        "    plt.yticks(fontsize=8)\n",
        "\n",
        "    # Only show legend on one key plot\n",
        "    if \"RMSEP\" in title and \"Variety\" in title:\n",
        "        plt.legend(fontsize=8, frameon=False)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# === Run evaluations for variety ===\n",
        "rmsep_variety, fractions = run_bootstrapped_uncertainty_eval(\n",
        "    df_uncertainty=df_bayesfc_variety,\n",
        "    model_dfs=model_dfs_variety,\n",
        "    model_names=models_to_plot,\n",
        "    metric=\"rmsep\"\n",
        ")\n",
        "\n",
        "acc_variety, _ = run_bootstrapped_uncertainty_eval(\n",
        "    df_uncertainty=df_bayesfc_variety,\n",
        "    model_dfs=model_dfs_variety,\n",
        "    model_names=models_to_plot,\n",
        "    metric=\"accuracy\"\n",
        ")\n",
        "\n",
        "# === Run evaluations for season ===\n",
        "rmsep_season, _ = run_bootstrapped_uncertainty_eval(\n",
        "    df_uncertainty=df_bayesfc_season,\n",
        "    model_dfs=model_dfs_season,\n",
        "    model_names=models_to_plot,\n",
        "    metric=\"rmsep\"\n",
        ")\n",
        "\n",
        "acc_season, _ = run_bootstrapped_uncertainty_eval(\n",
        "    df_uncertainty=df_bayesfc_season,\n",
        "    model_dfs=model_dfs_season,\n",
        "    model_names=models_to_plot,\n",
        "    metric=\"accuracy\"\n",
        ")\n",
        "\n",
        "# === Plot all metrics ===\n",
        "plot_bootstrap_metric(rmsep_variety, fractions, \"RMSEP vs. Uncertainty (Variety, 10 MC, 1000 Bootstraps)\", \"RMSEP\")\n",
        "plot_bootstrap_metric(acc_variety,  fractions, \"Practical Accuracy vs. Uncertainty (Variety, 10 MC, 1000 Bootstraps)\", \"Practical Accuracy\")\n",
        "plot_bootstrap_metric(rmsep_season, fractions, \"RMSEP vs. Uncertainty (Season, 10 MC, 1000 Bootstraps)\", \"RMSEP\")\n",
        "plot_bootstrap_metric(acc_season,  fractions, \"Practical Accuracy vs. Uncertainty (Season, 10 MC, 1000 Bootstraps)\", \"Practical Accuracy\")\n",
        "\n",
        "# === Print summary of uncertainty-performance correlation ===\n",
        "compute_bootstrap_spearman_correlations(rmsep_variety, fractions, \"rmsep\")\n",
        "compute_bootstrap_spearman_correlations(acc_variety,  fractions, \"accuracy\")\n",
        "compute_bootstrap_spearman_correlations(rmsep_season, fractions, \"rmsep\")\n",
        "compute_bootstrap_spearman_correlations(acc_season,  fractions, \"accuracy\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.16",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}