{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### _Setup_"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset memory\n",
        "%reset -f"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1754473964432
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Packages\n",
        "from typing  import Tuple\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cross_decomposition import PLSRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import KFold\n",
        "from typing import Union, List\n",
        "import time\n",
        "from kneed import KneeLocator\n",
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1754473965006
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Data\n",
        "df = pd.read_csv('data.csv')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1754474009297
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### _Functions_"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_col_index_of_spectra(\n",
        "    df: pd.DataFrame\n",
        ") -> int:\n",
        "    \"\"\"\n",
        "    Find the column index where spectral data starts.\n",
        "\n",
        "    Assumes spectral column names can be converted to float (e.g., \"730.5\", \"731.0\").\n",
        "\n",
        "    Parameters:\n",
        "        df : Input DataFrame\n",
        "\n",
        "    Returns:\n",
        "        Index of the first spectral column, or -1 if not found.\n",
        "    \"\"\"\n",
        "    for idx, col in enumerate(df.columns):\n",
        "        try:\n",
        "            float(col)\n",
        "            return idx\n",
        "        except (ValueError, TypeError):\n",
        "            continue\n",
        "    return -1\n",
        "\n",
        "def split_train_test(\n",
        "    df: pd.DataFrame,\n",
        "    test_variety: str,\n",
        "    test_season: int       \n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Split a DataFrame into one training set and two test sets:\n",
        "\n",
        "    - Variety test set: Variety == test_variety AND Year == 2024\n",
        "    - Season test set : Year == test_season \n",
        "\n",
        "    The training set excludes all rows that belong to any of the test sets.\n",
        "    The season test set only includes varieties that are present in the training set.\n",
        "\n",
        "    Parameters:\n",
        "        df           : Full pandas DataFrame\n",
        "        test_variety : Variety used for the test set\n",
        "        test_season  : Year used for the season test\n",
        "\n",
        "    Returns:\n",
        "        df_train        : Training set\n",
        "        df_test_variety : Test set for specified variety and 2024\n",
        "        df_test_season  : Test set for specified season (filtered by train varieties)\n",
        "    \"\"\"\n",
        "\n",
        "    # Select test set for the specified variety in year 2024\n",
        "    df_test_variety = df[\n",
        "        (df[\"Variety\"] == test_variety) &\n",
        "        (df[\"Scan Date Year\"] == 2024)\n",
        "    ]\n",
        "\n",
        "    # Select test set for the specified season (regardless of variety)\n",
        "    df_test_season = df[\n",
        "        df[\"Scan Date Year\"] == test_season\n",
        "    ]\n",
        "\n",
        "    # Select training set (exclude test variety and test season)\n",
        "    df_train = df[\n",
        "        (df[\"Variety\"] != test_variety) &\n",
        "        (df[\"Scan Date Year\"] != test_season)\n",
        "    ]\n",
        "\n",
        "    # Filter season test set to only include varieties present in training set\n",
        "    train_varieties = df_train[\"Variety\"].unique()\n",
        "    df_test_season = df_test_season[\n",
        "        df_test_season[\"Variety\"].isin(train_varieties)\n",
        "    ]\n",
        "\n",
        "    return df_train, df_test_variety, df_test_season\n",
        "\n",
        "def split_x_y(\n",
        "    df: pd.DataFrame,\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Split train and test DataFrames into x (spectra features) and y (target) arrays.\n",
        "    Assumes find_col_index_of_spectra() is defined globally.\n",
        "\n",
        "    Parameters:\n",
        "        df_train: Training set DataFrame.\n",
        "        df_test : Test set DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        x_train: NumPy array of training features.\n",
        "        y_train: NumPy array of training targets.\n",
        "    \"\"\"\n",
        "    spectra_cols = list(df.columns[find_col_index_of_spectra(df):])\n",
        "    target_cols = ['Brix (Position)']\n",
        "\n",
        "    x = df[spectra_cols].values\n",
        "    y = df[target_cols].values\n",
        "\n",
        "    return (\n",
        "        x,\n",
        "        y\n",
        "    )\n",
        "\n",
        "def stratified_cv_splits(\n",
        "    x_train, \n",
        "    y_train, \n",
        "    n_splits, \n",
        "    random_state, \n",
        "    n_bins=10\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate stratified K-Fold splits for regression by binning y_train into quantiles.\n",
        "\n",
        "    Parameters:\n",
        "        x_train      : Feature matrix (NumPy array or DataFrame)\n",
        "        y_train      : Target values (NumPy array or Series)\n",
        "        n_splits     : Number of folds for StratifiedKFold\n",
        "        random_state : Random seed for reproducibility\n",
        "        n_bins       : Number of quantile bins to stratify target into (default: 10)\n",
        "\n",
        "    Returns:\n",
        "        Generator of (train_idx, val_idx) tuples\n",
        "    \"\"\"\n",
        "\n",
        "    # Bin the target into quantiles\n",
        "    y_binned = pd.qcut(np.ravel(y_train), q=n_bins, labels=False, duplicates='drop')\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
        "\n",
        "    for train_idx, val_idx in skf.split(x_train, y_binned):\n",
        "        yield train_idx, val_idx\n",
        "\n",
        "def train_cross_validation(\n",
        "    x_train_data: np.ndarray,\n",
        "    y_train_data: np.ndarray,\n",
        "    max_components: int,\n",
        "    n_splits: int,\n",
        "    random_state: int,\n",
        ") -> Tuple[list[float], float, int]:\n",
        "    \"\"\"\n",
        "    Perform stratified K-fold cross-validation for PLS regression using different numbers\n",
        "    of components, and return the RMSECV list, the RMSECV at knee point,\n",
        "    and the corresponding number of components.\n",
        "\n",
        "    Parameters:\n",
        "        x_train_data  : Feature array for training (NumPy array)\n",
        "        y_train_data  : Target array for training (NumPy array)\n",
        "        max_components: Maximum number of PLS components to evaluate\n",
        "        n_splits      : Number of stratified folds in cross-validation\n",
        "        random_state  : Seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "        cv_rmsecv     : List of RMSECV scores per component count\n",
        "        cv_opt_rmsecv : RMSECV at the selected optimal number of components\n",
        "        cv_opt_A      : Optimal number of components (selected at knee point or min RMSECV)\n",
        "    \"\"\"\n",
        "    cv_rmsecv = []\n",
        "\n",
        "    # Loop over 1 to max_components\n",
        "    for n_comp in range(1, max_components + 1):\n",
        "        mse_folds = []\n",
        "\n",
        "        # Stratified K-fold CV splits\n",
        "        for train_idx, val_idx in stratified_cv_splits(x_train_data, y_train_data, n_splits, random_state):\n",
        "            # Split data into current train/val fold\n",
        "            X_train_fold = x_train_data[train_idx]\n",
        "            X_val_fold   = x_train_data[val_idx]\n",
        "            y_train_fold = y_train_data[train_idx]\n",
        "            y_val_fold   = y_train_data[val_idx]\n",
        "\n",
        "            # Train PLS with current number of components\n",
        "            pls = PLSRegression(n_components=n_comp)\n",
        "            pls.fit(X_train_fold, y_train_fold)\n",
        "\n",
        "            # Predict on validation fold and store MSE\n",
        "            y_pred_val = pls.predict(X_val_fold)\n",
        "            mse_folds.append(mean_squared_error(y_val_fold, y_pred_val))\n",
        "\n",
        "        # Compute RMSECV for current component count\n",
        "        rmsecv = np.sqrt(np.mean(mse_folds))\n",
        "        cv_rmsecv.append(rmsecv)\n",
        "        print(f\"Components: {n_comp}, RMSECV: {rmsecv:.4f}\")\n",
        "\n",
        "    # === Plot RMSECV curve ===\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(range(1, max_components + 1), cv_rmsecv, marker='o')\n",
        "    plt.xlabel('Number of Components')\n",
        "    plt.ylabel('RMSECV')\n",
        "    plt.title('RMSECV vs Number of Components')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # === Find optimal number of components using knee/elbow method ===\n",
        "    x = list(range(1, max_components + 1))\n",
        "    y = cv_rmsecv\n",
        "\n",
        "    knee_locator = KneeLocator(x, y, curve='convex', direction='decreasing')\n",
        "    cv_opt_A = knee_locator.knee\n",
        "\n",
        "    # Fallback: use component with lowest RMSECV if knee not found\n",
        "    if cv_opt_A is None:\n",
        "        print(\"Knee not found automatically. Using minimum RMSECV as fallback.\")\n",
        "        cv_opt_A = np.argmin(cv_rmsecv) + 1\n",
        "\n",
        "    # Get RMSECV value at the selected number of components\n",
        "    cv_opt_rmsecv = cv_rmsecv[cv_opt_A - 1] \n",
        "\n",
        "    return (\n",
        "        cv_rmsecv,\n",
        "        cv_opt_rmsecv,\n",
        "        cv_opt_A\n",
        "    )\n",
        "\n",
        "def test_pls_model(\n",
        "    x_train_data: np.ndarray,\n",
        "    y_train_data: np.ndarray,\n",
        "    x_test_data: np.ndarray,\n",
        "    y_test_data: np.ndarray,\n",
        "    opt_A: int,\n",
        ") -> Tuple[\n",
        "    PLSRegression,      # trained model\n",
        "    pd.DataFrame,       # test_pls_results\n",
        "    float,              # test_rmsep\n",
        "    float,              # test_r2\n",
        "    float               # test_practical_accuracy\n",
        "]:\n",
        "    \"\"\"\n",
        "    Train a final PLS regression model on the training set using the specified number \n",
        "    of latent variables, then evaluate its performance on the test set.\n",
        "\n",
        "    Parameters:\n",
        "        x_train_data : NumPy array of training features.\n",
        "        y_train_data : NumPy array of training target values.\n",
        "        x_test_data  : NumPy array of test features.\n",
        "        y_test_data  : NumPy array of test target values.\n",
        "        opt_A        : Optimal number of latent variables (PLS components) to use.\n",
        "\n",
        "    Returns:\n",
        "        pls_model                 : The trained PLS model.\n",
        "        test_pls_results          : DataFrame with columns [\"observed\", \"predicted\"].\n",
        "        test_rmsep                : Root Mean Squared Error of Prediction on the test set.\n",
        "        test_r2                   : R² score on the test set.\n",
        "        test_practical_accuracy   : % of predictions within 20% of actual.\n",
        "    \"\"\"\n",
        "\n",
        "    # === Train final PLS model with selected number of components ===\n",
        "    pls_model = PLSRegression(n_components=opt_A)\n",
        "    pls_model.fit(x_train_data, y_train_data)\n",
        "\n",
        "    # === Predict on test set ===\n",
        "    y_pred_test = pls_model.predict(x_test_data).flatten()\n",
        "\n",
        "    # === Flatten ground truth and prediction arrays ===\n",
        "    y_true = y_test_data.flatten()\n",
        "    y_pred = y_pred_test.flatten()\n",
        "\n",
        "    # === Compute test metrics ===\n",
        "    test_mse = mean_squared_error(y_true, y_pred)\n",
        "    test_rmsep = np.sqrt(test_mse)\n",
        "    test_r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    # === Compute practical accuracy (±20% tolerance) ===\n",
        "    pct_error = np.abs(y_pred - y_true) / np.abs(y_true)\n",
        "    test_practical_accuracy = np.mean(pct_error <= 0.2) * 100.0\n",
        "\n",
        "    # === Compile results into DataFrame ===\n",
        "    test_pls_results = pd.DataFrame({\n",
        "        \"observed\": y_true,\n",
        "        \"predicted\": y_pred\n",
        "    })\n",
        "\n",
        "    # === Print test summary ===\n",
        "    print(f\"Test RMSEP: {test_rmsep:.4f}\")\n",
        "    print(f\"Test R²: {test_r2:.4f}\")\n",
        "    print(f\"Practical accuracy (±20%): {test_practical_accuracy:.1f}%\")\n",
        "\n",
        "    # === Plot observed vs. predicted values ===\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(y_true, y_pred, alpha=0.7, label=\"Test Data\")\n",
        "    plt.plot(\n",
        "        [y_true.min(), y_true.max()],\n",
        "        [y_true.min(), y_true.max()],\n",
        "        'k--', lw=2, label=\"Ideal\"\n",
        "    )\n",
        "    plt.xlabel(\"Observed\")\n",
        "    plt.ylabel(\"Predicted\")\n",
        "    plt.title(\"Observed vs. Predicted on Test Set\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # === Return model, predictions, and evaluation metrics ===\n",
        "    return (\n",
        "        pls_model,\n",
        "        test_pls_results,\n",
        "        test_rmsep,\n",
        "        test_r2,\n",
        "        test_practical_accuracy\n",
        "    )\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1754474009361
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### _Variables_"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DF = df\n",
        "RANDOM_STATE = 27                                               \n",
        "TEST_VARIETY = \"TestVariety\"\n",
        "TEST_SEASON = 2025\n",
        "MAX_COMPONENTS = 20\n",
        "N_SPLITS = 3"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1754474010930
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### _Run_"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Split into train and test sets ===\n",
        "df_train, df_test_variety, df_test_season = split_train_test(\n",
        "    df,\n",
        "    test_variety=TEST_VARIETY,\n",
        "    test_season=TEST_SEASON,\n",
        ")\n",
        "\n",
        "# === Convert to x and y arrays ===\n",
        "x_train, y_train = split_x_y(\n",
        "    df_train,\n",
        ")\n",
        "x_test_variety, y_train_variety = split_x_y(\n",
        "    df_test_variety,\n",
        ")\n",
        "x_test_season, y_train_season = split_x_y(\n",
        "    df_test_season,\n",
        ")\n",
        "\n",
        "# === Perform k-fold cross-validation ===\n",
        "cv_rmsecv, cv_min_rmsecv, cv_opt_A = train_cross_validation(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    max_components=MAX_COMPONENTS,\n",
        "    n_splits=N_SPLITS,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "print(f\"Optimal # of components: {cv_opt_A} | CV RMSECV: {cv_min_rmsecv:.4f}\")\n",
        "\n",
        "# === Train final model and evaluate on test sets ===\n",
        "pls_model_variety, results_variety, rmsep_variety, r2_variety, acc_variety = test_pls_model(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    x_test_variety,\n",
        "    y_train_variety,\n",
        "    opt_A=cv_opt_A\n",
        ")\n",
        "\n",
        "pls_model_season, results_season, rmsep_season, r2_season, acc_season = test_pls_model(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    x_test_season,\n",
        "    y_train_season,\n",
        "    opt_A=cv_opt_A\n",
        ")\n",
        "\n",
        "# === Print summary ===\n",
        "print(\"\\n--- Test Results Summary ---\")\n",
        "print(f\"Variety Test  | RMSEP: {rmsep_variety:.4f} | R²: {r2_variety:.4f} | Acc: {acc_variety:.2f}\")\n",
        "print(f\"Season Test   | RMSEP: {rmsep_season:.4f}  | R²: {r2_season:.4f}  | Acc: {acc_season:.2f}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1754474025337
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### _Inference Time Analysis_"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_inference_sample_set(\n",
        "    df_variety: pd.DataFrame,\n",
        "    df_season: pd.DataFrame,\n",
        "    random_state: int,\n",
        "    sample_size: int = 1000\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Combine two test sets (variety and season), sample rows randomly, and return X and y arrays.\n",
        "\n",
        "    Parameters:\n",
        "        df_variety   : DataFrame for variety-based test set\n",
        "        df_season    : DataFrame for season-based test set\n",
        "        random_state : Random seed for reproducibility\n",
        "        sample_size  : Number of rows to sample from combined test set\n",
        "\n",
        "    Returns:\n",
        "        x_sample : NumPy array of shape (sample_size, n_features) with spectral features\n",
        "        y_sample : NumPy array of shape (sample_size,) with corresponding Brix values\n",
        "    \"\"\"\n",
        "    # Combine the two test sets\n",
        "    df_combined = pd.concat([df_variety, df_season], axis=0)\n",
        "\n",
        "    # Randomly sample rows from the combined test set\n",
        "    df_sample = df_combined.sample(\n",
        "        n=sample_size,\n",
        "        random_state=random_state\n",
        "    )\n",
        "\n",
        "    # Split into X and y arrays\n",
        "    x_sample, y_sample = split_x_y(df_sample)\n",
        "\n",
        "    return x_sample, y_sample\n",
        "\n",
        "def test_pls_inference_time(\n",
        "    x_train: np.ndarray,\n",
        "    y_train: np.ndarray,\n",
        "    x_test: np.ndarray,\n",
        "    y_test: np.ndarray,\n",
        "    n_components: int\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Train PLS with given number of components and return average inference time per sample (in ms).\n",
        "\n",
        "    Parameters:\n",
        "        x_train      : Training features\n",
        "        y_train      : Training targets\n",
        "        x_test       : Test features\n",
        "        y_test       : Test targets\n",
        "        n_components : Number of PLS components (LVs)\n",
        "\n",
        "    Returns:\n",
        "        avg_inference_time_ms : Average inference time per sample in milliseconds\n",
        "    \"\"\"\n",
        "    # Fit the PLS model\n",
        "    pls = PLSRegression(n_components=n_components)\n",
        "    pls.fit(x_train, y_train)\n",
        "\n",
        "    times = []\n",
        "\n",
        "    # Predict each sample individually and time it\n",
        "    for x in x_test:\n",
        "        x_input = x.reshape(1, -1)\n",
        "        start = time.time()\n",
        "        _ = pls.predict(x_input)\n",
        "        end = time.time()\n",
        "        times.append(end - start)\n",
        "\n",
        "    # Compute average inference time in milliseconds\n",
        "    avg_inference_time_ms = np.mean(times) * 1000\n",
        "\n",
        "    # Print only the inference time\n",
        "    print(f\"Average inference time: {avg_inference_time_ms:.3f} ms/sample\")\n",
        "\n",
        "    return avg_inference_time_ms"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1754474025392
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Create sample set for inference time measurement ===\n",
        "x_inference_time, y_inference_time = get_inference_sample_set(\n",
        "    df_test_variety,\n",
        "    df_test_season,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "# === Compute the average inference time ===\n",
        "inference_time_ms = test_pls_inference_time(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    x_inference_time,\n",
        "    y_inference_time,\n",
        "    n_components=cv_opt_A\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1754474028931
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.16",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}