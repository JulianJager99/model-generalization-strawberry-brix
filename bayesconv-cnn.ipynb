{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### _Setup_"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset memory\n",
        "%reset -f"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1754484001503
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install correct package versions\n",
        "!pip install \"tensorflow[and-cuda]\"\n",
        "!pip uninstall numpy pandas -y\n",
        "!pip install \"numpy<2.0\" pandas --upgrade --no-cache-dir"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1754483839324
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Packages\n",
        "from typing import Union, List, Tuple, Dict, Any\n",
        "import time\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold, StratifiedKFold, StratifiedShuffleSplit, train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers, initializers, optimizers, callbacks\n",
        "from tensorflow.keras.layers import Layer\n",
        "import tensorflow_probability as tfp\n",
        "import optuna\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tfpl = tfp.layers\n",
        "tfd = tfp.distributions"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1754484147569
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU check\n",
        "print(\"Available GPUs:\", tf.config.list_physical_devices('GPU'))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1754484147645
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data\n",
        "df = pd.read_csv('data.csv')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1754484175913
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### _Functions_"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_col_index_of_spectra(\n",
        "    df: pd.DataFrame\n",
        ") -> int:\n",
        "    \"\"\"\n",
        "    Find the column index where spectral data starts.\n",
        "\n",
        "    Assumes spectral column names can be converted to float (e.g., \"730.5\", \"731.0\").\n",
        "\n",
        "    Parameters:\n",
        "        df : Input DataFrame\n",
        "\n",
        "    Returns:\n",
        "        Index of the first spectral column, or -1 if not found.\n",
        "    \"\"\"\n",
        "    for idx, col in enumerate(df.columns):\n",
        "        try:\n",
        "            float(col)\n",
        "            return idx\n",
        "        except (ValueError, TypeError):\n",
        "            continue\n",
        "    return -1\n",
        "\n",
        "def split_train_test(\n",
        "    df: pd.DataFrame,\n",
        "    test_variety: str,\n",
        "    test_season: int       \n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Split a DataFrame into one training set and two test sets:\n",
        "\n",
        "    - Variety test set: Variety == test_variety AND Year == 2024\n",
        "    - Season test set : Year == test_season \n",
        "\n",
        "    The training set excludes all rows that belong to any of the test sets.\n",
        "    The season test set only includes varieties that are present in the training set.\n",
        "\n",
        "    Parameters:\n",
        "        df           : Full pandas DataFrame\n",
        "        test_variety : Variety used for the test set\n",
        "        test_season  : Year used for the season test\n",
        "\n",
        "    Returns:\n",
        "        df_train        : Training set\n",
        "        df_test_variety : Test set for specified variety and 2024\n",
        "        df_test_season  : Test set for specified season (filtered by train varieties)\n",
        "    \"\"\"\n",
        "\n",
        "    # Select test set for the specified variety in year 2024\n",
        "    df_test_variety = df[\n",
        "        (df[\"Variety\"] == test_variety) &\n",
        "        (df[\"Scan Date Year\"] == 2024)\n",
        "    ]\n",
        "\n",
        "    # Select test set for the specified season (regardless of variety)\n",
        "    df_test_season = df[\n",
        "        df[\"Scan Date Year\"] == test_season\n",
        "    ]\n",
        "\n",
        "    # Select training set (exclude test variety and test season)\n",
        "    df_train = df[\n",
        "        (df[\"Variety\"] != test_variety) &\n",
        "        (df[\"Scan Date Year\"] != test_season)\n",
        "    ]\n",
        "\n",
        "    # Filter season test set to only include varieties present in training set\n",
        "    train_varieties = df_train[\"Variety\"].unique()\n",
        "    df_test_season = df_test_season[\n",
        "        df_test_season[\"Variety\"].isin(train_varieties)\n",
        "    ]\n",
        "\n",
        "    return df_train, df_test_variety, df_test_season\n",
        "\n",
        "def split_x_y(\n",
        "    df: pd.DataFrame,\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Split a DataFrame into X (spectral features) and y (target) arrays.\n",
        "    Assumes find_col_index_of_spectra() is defined globally and returns the index\n",
        "    where spectral data starts.\n",
        "\n",
        "    Parameters:\n",
        "        df : Input DataFrame containing both metadata and spectral data.\n",
        "\n",
        "    Returns:\n",
        "        x : NumPy array of shape (n_samples, n_spectral_features)\n",
        "        y : NumPy array of shape (n_samples, 1) containing Brix values\n",
        "    \"\"\"\n",
        "    # Identify spectral columns (those that can be cast to float, e.g. wavelengths)\n",
        "    spectra_cols = list(df.columns[find_col_index_of_spectra(df):])\n",
        "\n",
        "    # Define the target column\n",
        "    target_cols = ['Brix (Position)']\n",
        "\n",
        "    # Extract feature and target arrays\n",
        "    x = df[spectra_cols].values\n",
        "    y = df[target_cols].values\n",
        "\n",
        "    return x, y\n",
        "\n",
        "def take_subset(\n",
        "    df: pd.DataFrame, \n",
        "    n_subset: int,\n",
        "    random_state: int\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Return a stratified subset of the DataFrame based on 10 Brix bins.\n",
        "\n",
        "    If n_subset >= len(df), the original DataFrame is returned.\n",
        "\n",
        "    Parameters:\n",
        "        df       : Input DataFrame with 'Brix (Position)' column\n",
        "        n_subset : Desired subset size\n",
        "        random_state : Random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "        Subset of df with stratification over 10 quantile bins of Brix\n",
        "    \"\"\"\n",
        "    # If requested subset size exceeds full dataset, return a copy of the full DataFrame\n",
        "    if n_subset >= len(df):\n",
        "        return df.copy()\n",
        "\n",
        "    # Bin the Brix values into 10 quantile-based bins for stratification\n",
        "    binned = pd.qcut(df[\"Brix (Position)\"], q=10, labels=False, duplicates='drop')\n",
        "\n",
        "    # Initialize stratified sampler\n",
        "    splitter = StratifiedShuffleSplit(\n",
        "        n_splits=1,\n",
        "        train_size=n_subset,\n",
        "        random_state=random_state\n",
        "    )\n",
        "\n",
        "    # Perform stratified split and extract subset indices\n",
        "    idx_subset, _ = next(splitter.split(df, binned))\n",
        "\n",
        "    # Return the stratified subset as a new DataFrame with reset index\n",
        "    return df.iloc[idx_subset].reset_index(drop=True)\n",
        "\n",
        "def create_train_val_split(\n",
        "    df: pd.DataFrame,\n",
        "    validation_size: float,\n",
        "    random_state: int\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Split a DataFrame into train and validation sets using stratified sampling\n",
        "    based on 10 quantile bins of the 'Brix (Position)' column.\n",
        "\n",
        "    Parameters:\n",
        "        df              : Input DataFrame\n",
        "        validation_size : Proportion of validation samples (0 < float < 1)\n",
        "        random_state    : Seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "        df_train, df_val : Stratified training and validation DataFrames\n",
        "    \"\"\"\n",
        "    # Bin the Brix values into 10 quantile-based bins for stratified splitting\n",
        "    binned = pd.qcut(df[\"Brix (Position)\"], q=10, labels=False, duplicates=\"drop\")\n",
        "\n",
        "    # Perform stratified train/validation split based on the binned Brix values\n",
        "    df_train, df_val = train_test_split(\n",
        "        df,\n",
        "        test_size=validation_size,\n",
        "        random_state=random_state,\n",
        "        stratify=binned\n",
        "    )\n",
        "\n",
        "    # Return splits with reset indices\n",
        "    return df_train.reset_index(drop=True), df_val.reset_index(drop=True)\n",
        "\n",
        "def rmse_loss(\n",
        "    y_true, \n",
        "    y_pred\n",
        "):\n",
        "    \"\"\"\n",
        "    Compute the Root Mean Squared Error (RMSE) as a loss function.\n",
        "\n",
        "    Parameters:\n",
        "        y_true : Tensor of true target values\n",
        "        y_pred : Tensor of predicted values\n",
        "\n",
        "    Returns:\n",
        "        RMSE as a scalar Tensor\n",
        "    \"\"\"\n",
        "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))  \n",
        "\n",
        "def rmse_metric(\n",
        "    y_true, \n",
        "    y_pred\n",
        "):\n",
        "    \"\"\"\n",
        "    Compute the Root Mean Squared Error (RMSE) as a performance metric.\n",
        "\n",
        "    Parameters:\n",
        "        y_true : Tensor of true target values\n",
        "        y_pred : Tensor of predicted values\n",
        "\n",
        "    Returns:\n",
        "        RMSE as a scalar Tensor\n",
        "    \"\"\"\n",
        "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))  \n",
        "\n",
        "class FusedLassoPrior(tfd.Distribution):\n",
        "    \"\"\"\n",
        "    Custom Fused Lasso prior distribution for use in Bayesian neural networks.\n",
        "\n",
        "    Combines:\n",
        "    - L1 regularization (lasso) to encourage sparsity\n",
        "    - Fused penalty to encourage smoothness across adjacent coefficients\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        shape,\n",
        "        lambda_l1,\n",
        "        lambda_fused,\n",
        "        validate_args=False,\n",
        "        allow_nan_stats=True\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the fused lasso distribution.\n",
        "\n",
        "        Parameters:\n",
        "            shape           : Shape of the distribution's support\n",
        "            lambda_l1       : Strength of L1 regularization\n",
        "            lambda_fused    : Strength of fused (difference) penalty\n",
        "            validate_args   : Whether to validate distribution arguments\n",
        "            allow_nan_stats : Whether to allow NaNs in statistical outputs\n",
        "        \"\"\"\n",
        "        super().__init__(\n",
        "            dtype=tf.float32,\n",
        "            reparameterization_type=tfd.NOT_REPARAMETERIZED,\n",
        "            validate_args=validate_args,\n",
        "            allow_nan_stats=allow_nan_stats\n",
        "        )\n",
        "        self.lambda_l1 = lambda_l1\n",
        "        self.lambda_fused = lambda_fused\n",
        "        self._shape = tf.TensorShape(shape)\n",
        "\n",
        "    def _log_prob(self, value):\n",
        "        \"\"\"\n",
        "        Compute the unnormalized log-probability of a sample.\n",
        "\n",
        "        Applies:\n",
        "        - L1 penalty: sum of absolute values\n",
        "        - Fused penalty: sum of absolute differences between adjacent elements\n",
        "        \"\"\"\n",
        "        l1_term = tf.reduce_sum(tf.abs(value))\n",
        "        fused_term = tf.reduce_sum(tf.abs(value[..., 1:] - value[..., :-1]))\n",
        "        return -self.lambda_l1 * l1_term - self.lambda_fused * fused_term\n",
        "\n",
        "    def _batch_shape(self):\n",
        "        # No explicit batch shape; return total shape\n",
        "        return self._shape\n",
        "\n",
        "    def _event_shape(self):\n",
        "        # Event shape is the full shape of the variable\n",
        "        return self._shape\n",
        "\n",
        "def fused_lasso_prior_fn(lambda_l1, lambda_fused):\n",
        "    \"\"\"\n",
        "    Returns a callable prior function for use in Bayesian layers.\n",
        "\n",
        "    This function is designed to be passed to a Bayesian layer (e.g. tfp.layers.DenseVariational),\n",
        "    where it constructs a FusedLassoPrior distribution for each weight tensor.\n",
        "\n",
        "    Parameters:\n",
        "        lambda_l1    : Strength of the L1 sparsity penalty\n",
        "        lambda_fused : Strength of the fused smoothness penalty\n",
        "\n",
        "    Returns:\n",
        "        fn : A callable that returns a FusedLassoPrior when given a shape\n",
        "    \"\"\"\n",
        "\n",
        "    def fn(\n",
        "        dtype=tf.float32,\n",
        "        shape=None,\n",
        "        name=None,\n",
        "        trainable=True,\n",
        "        add_variable_fn=None\n",
        "    ):\n",
        "        # Shape is required to build the prior distribution\n",
        "        if shape is None:\n",
        "            raise ValueError(\"Shape must be provided to construct the FusedLassoPrior.\")\n",
        "        \n",
        "        # Return a FusedLassoPrior distribution for the given shape\n",
        "        return FusedLassoPrior(\n",
        "            shape=shape,\n",
        "            lambda_l1=lambda_l1,\n",
        "            lambda_fused=lambda_fused\n",
        "        )\n",
        "\n",
        "    return fn\n",
        "\n",
        "def bcnn_model(\n",
        "    input_shape: int,\n",
        "    kernel_size: int,\n",
        "    dropout_rate: float,\n",
        "    l2_strength: float,\n",
        "    learning_rate: float,\n",
        "    random_state: int,\n",
        "    kl_scale: float,\n",
        "    lambda_l1: float,\n",
        "    lambda_fused: float\n",
        ") -> tf.keras.Model:\n",
        "    \"\"\"\n",
        "    Builds and compiles a Bayesian CNN using a fused lasso prior on the convolutional weights.\n",
        "\n",
        "    Parameters:\n",
        "        input_shape   : Number of input features (spectral length).\n",
        "        kernel_size   : Size of the 1D convolutional kernel.\n",
        "        dropout_rate  : Dropout rate for regularization.\n",
        "        l2_strength   : L2 regularization strength applied to Dense layers.\n",
        "        learning_rate : Learning rate for the Adam optimizer.\n",
        "        random_state  : Seed used for weight initialization.\n",
        "        kl_scale      : Scale factor for KL divergence loss.\n",
        "        lambda_l1     : Weight of L1 sparsity term in fused lasso prior.\n",
        "        lambda_fused  : Weight of smoothness term in fused lasso prior.\n",
        "\n",
        "    Returns:\n",
        "        model : A compiled Keras model ready for training and evaluation.\n",
        "    \"\"\"\n",
        "    # Define kernel regularizer and initializer\n",
        "    kernel_reg  = regularizers.l2(l2_strength)\n",
        "    kernel_init = initializers.HeNormal(seed=random_state)\n",
        "\n",
        "    # Build sequential model\n",
        "    model = models.Sequential([\n",
        "        # Input layer: Reshape flat vector to (length, 1) for Conv1D\n",
        "        tf.keras.Input(shape=(input_shape,)),\n",
        "        layers.Reshape((input_shape, 1)),\n",
        "\n",
        "        # Bayesian Conv1D with fused lasso prior\n",
        "        tfpl.Convolution1DReparameterization(\n",
        "            filters=1,\n",
        "            kernel_size=kernel_size,\n",
        "            padding=\"same\",\n",
        "            activation=\"elu\",\n",
        "            kernel_posterior_fn=tfpl.default_mean_field_normal_fn(),\n",
        "            kernel_prior_fn=fused_lasso_prior_fn(lambda_l1=lambda_l1, lambda_fused=lambda_fused),\n",
        "            kernel_divergence_fn=lambda q, p, _: tfd.kl_divergence(q, p) * kl_scale,\n",
        "            bias_posterior_fn=tfpl.default_mean_field_normal_fn(is_singular=False),\n",
        "            bias_prior_fn=tfpl.default_multivariate_normal_fn,\n",
        "            bias_divergence_fn=lambda q, p, _: tfd.kl_divergence(q, p) * kl_scale,\n",
        "        ),\n",
        "\n",
        "        # Dropout after convolution\n",
        "        layers.Dropout(dropout_rate),\n",
        "\n",
        "        # Flatten for Dense layers\n",
        "        layers.Flatten(),\n",
        "\n",
        "        # Dense regression head with dropout and L2 regularization\n",
        "        layers.Dense(36, activation=\"elu\", kernel_initializer=kernel_init, kernel_regularizer=kernel_reg),\n",
        "        layers.Dropout(dropout_rate),\n",
        "        layers.Dense(18, activation=\"elu\", kernel_initializer=kernel_init, kernel_regularizer=kernel_reg),\n",
        "        layers.Dropout(dropout_rate),\n",
        "        layers.Dense(12, activation=\"elu\", kernel_initializer=kernel_init, kernel_regularizer=kernel_reg),\n",
        "\n",
        "        # Output layer (regression target)\n",
        "        layers.Dense(1, activation=\"linear\", kernel_initializer=kernel_init, kernel_regularizer=kernel_reg),\n",
        "    ])\n",
        "\n",
        "    # Compile model with RMSE loss and metric\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "        loss=lambda y_true, y_pred: tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true))),\n",
        "        metrics=[lambda y_true, y_pred: tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "def train_bcnn(\n",
        "    x_train: np.ndarray,\n",
        "    y_train: np.ndarray,\n",
        "    input_shape: int,\n",
        "    kernel_size: int,\n",
        "    dropout_rate: float,\n",
        "    l2_strength: float,\n",
        "    random_state: int,\n",
        "    kl_scale: float,\n",
        "    batch_size: int,\n",
        "    epochs: int,\n",
        "    patience_reduce_lr: int,\n",
        "    patience_early_stop: int,\n",
        "    min_lr: float,\n",
        "    x_val: np.ndarray,\n",
        "    y_val: np.ndarray,\n",
        "    lambda_l1: float,\n",
        "    lambda_fused: float,\n",
        "    verbose: int = 0\n",
        ") -> Tuple[tf.keras.Model, tf.keras.callbacks.History]:\n",
        "    \"\"\"\n",
        "    Train a Bayesian CNN (BCNN) model with fused lasso prior.\n",
        "\n",
        "    If a validation set is provided, callbacks monitor 'val_loss'.\n",
        "    Otherwise, training is done without validation and callbacks monitor 'loss'.\n",
        "\n",
        "    Parameters:\n",
        "        x_train           : Training feature matrix\n",
        "        y_train           : Training target vector\n",
        "        input_shape       : Number of features per input sample\n",
        "        kernel_size       : Size of the convolutional kernel\n",
        "        dropout_rate      : Dropout rate for regularization\n",
        "        l2_strength       : L2 regularization strength for dense layers\n",
        "        random_state      : Random seed for reproducibility\n",
        "        kl_scale          : KL divergence scale for Bayesian regularization\n",
        "        batch_size        : Number of samples per gradient update\n",
        "        epochs            : Maximum number of training epochs\n",
        "        patience_reduce_lr: Patience for reducing LR on plateau\n",
        "        patience_early_stop: Patience for early stopping\n",
        "        min_lr            : Minimum learning rate during ReduceLROnPlateau\n",
        "        x_val             : Validation features (optional)\n",
        "        y_val             : Validation targets (optional)\n",
        "        lambda_l1         : L1 penalty coefficient in fused lasso prior\n",
        "        lambda_fused      : Fused penalty coefficient in fused lasso prior\n",
        "        verbose           : Keras verbosity mode (0, 1, or 2)\n",
        "\n",
        "    Returns:\n",
        "        model   : Trained BCNN Keras model\n",
        "        history : Keras training history object\n",
        "    \"\"\"\n",
        "\n",
        "    # Build the BCNN model using specified parameters\n",
        "    model = bcnn_model(\n",
        "        input_shape=input_shape,\n",
        "        kernel_size=kernel_size,\n",
        "        dropout_rate=dropout_rate,\n",
        "        l2_strength=l2_strength,\n",
        "        learning_rate=0.01 * batch_size / 256,  # Linear scaling rule\n",
        "        random_state=random_state,\n",
        "        kl_scale=kl_scale,\n",
        "        lambda_l1=lambda_l1,\n",
        "        lambda_fused=lambda_fused\n",
        "    )\n",
        "\n",
        "    # Determine monitoring target and validation data\n",
        "    if x_val is not None and y_val is not None:\n",
        "        monitor_metric = \"val_loss\"\n",
        "        validation_data = (x_val, y_val)\n",
        "    else:\n",
        "        monitor_metric = \"loss\"\n",
        "        validation_data = None\n",
        "\n",
        "    # Define training callbacks\n",
        "    cb = [\n",
        "        callbacks.ReduceLROnPlateau(\n",
        "            monitor=monitor_metric,\n",
        "            factor=0.5,\n",
        "            patience=patience_reduce_lr,\n",
        "            min_lr=min_lr,\n",
        "            verbose=0\n",
        "        ),\n",
        "        callbacks.EarlyStopping(\n",
        "            monitor=monitor_metric,\n",
        "            patience=patience_early_stop,\n",
        "            restore_best_weights=True,\n",
        "            verbose=0\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Fit the model\n",
        "    history = model.fit(\n",
        "        x_train,\n",
        "        y_train,\n",
        "        validation_data=validation_data,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        callbacks=cb,\n",
        "        verbose=verbose\n",
        "    )\n",
        "\n",
        "    return model, history\n",
        "\n",
        "def perform_optuna_hyperparameter_optimization(\n",
        "    x_train: np.ndarray,\n",
        "    y_train: np.ndarray,\n",
        "    x_val: np.ndarray,\n",
        "    y_val: np.ndarray,\n",
        "    input_shape: int,\n",
        "    random_state: int,\n",
        "    epochs: int,\n",
        "    patience_reduce_lr: int,\n",
        "    patience_early_stop: int,\n",
        "    min_lr: float,\n",
        "    kernel_size_range: Tuple[int, int],\n",
        "    batch_size_list: list,\n",
        "    dropout_range: Tuple[float, float],\n",
        "    l2_range: Tuple[float, float],\n",
        "    kl_range: Tuple[float, float],\n",
        "    lasso_range: Tuple[float, float],\n",
        "    timeout_time: float\n",
        ") -> Tuple['optuna.study.Study', float, dict]:\n",
        "    \"\"\"\n",
        "    Runs Optuna hyperparameter optimization for a Bayesian CNN with fused lasso priors.\n",
        "\n",
        "    Parameters:\n",
        "        x_train            : Training features\n",
        "        y_train            : Training targets\n",
        "        x_val              : Validation features\n",
        "        y_val              : Validation targets\n",
        "        input_shape        : Number of input features\n",
        "        random_state       : Seed for reproducibility\n",
        "        epochs             : Maximum number of training epochs\n",
        "        patience_reduce_lr : Patience for ReduceLROnPlateau\n",
        "        patience_early_stop: Patience for EarlyStopping\n",
        "        min_lr             : Minimum learning rate\n",
        "        kernel_size_range  : Tuple (min, max) for kernel size\n",
        "        batch_size_list    : List of possible batch sizes\n",
        "        dropout_range      : Tuple (min, max) for dropout rate\n",
        "        l2_range           : Tuple (min, max) for L2 regularization\n",
        "        lr_range           : [Unused] Learning rate is scaled from batch size\n",
        "        kl_range           : Tuple (min, max) for KL divergence weight\n",
        "        lasso_range        : Tuple (min, max) for lambda_l1 and lambda_fused\n",
        "        timeout_time       : Maximum search time for Optuna (in seconds)\n",
        "\n",
        "    Returns:\n",
        "        study          : Optuna study object\n",
        "        best_val_rmse  : Best validation RMSE found\n",
        "        best_params    : Dictionary of best hyperparameters\n",
        "    \"\"\"\n",
        "\n",
        "    def objective(trial):\n",
        "        # Sample hyperparameters from the defined search space\n",
        "        kernel_size   = trial.suggest_int(\"kernel_size\", kernel_size_range[0], kernel_size_range[1])\n",
        "        batch_size    = trial.suggest_categorical(\"batch_size\", batch_size_list)\n",
        "        dropout_rate  = trial.suggest_float(\"dropout_rate\", dropout_range[0], dropout_range[1])\n",
        "        l2_strength   = trial.suggest_float(\"l2_strength\", l2_range[0], l2_range[1], log=True)\n",
        "        kl_scale      = trial.suggest_float(\"kl_scale\", kl_range[0], kl_range[1], log=True)\n",
        "        lambda_l1     = trial.suggest_float(\"lambda_l1\", lasso_range[0], lasso_range[1], log=True)\n",
        "        lambda_fused  = trial.suggest_float(\"lambda_fused\", lasso_range[0], lasso_range[1], log=True)\n",
        "\n",
        "        # Use linear scaling rule for learning rate\n",
        "        learning_rate = 0.01 * batch_size / 256\n",
        "\n",
        "        # Log selected trial parameters\n",
        "        print(f\"\\n[Optuna Trial {trial.number}] Hyperparameters:\")\n",
        "        print(f\"  kernel_size   = {kernel_size}\")\n",
        "        print(f\"  batch_size    = {batch_size}\")\n",
        "        print(f\"  dropout_rate  = {dropout_rate:.4f}\")\n",
        "        print(f\"  l2_strength   = {l2_strength:.2e}\")\n",
        "        print(f\"  learning_rate = {learning_rate:.2e}\")\n",
        "        print(f\"  kl_scale      = {kl_scale:.2e}\")\n",
        "        print(f\"  lambda_l1     = {lambda_l1:.2e}\")\n",
        "        print(f\"  lambda_fused  = {lambda_fused:.2e}\")\n",
        "\n",
        "        # Train BCNN model with sampled parameters\n",
        "        model, history = train_bcnn(\n",
        "            x_train=x_train,\n",
        "            y_train=y_train,\n",
        "            x_val=x_val,\n",
        "            y_val=y_val,\n",
        "            input_shape=input_shape,\n",
        "            kernel_size=kernel_size,\n",
        "            dropout_rate=dropout_rate,\n",
        "            l2_strength=l2_strength,\n",
        "            random_state=random_state,\n",
        "            kl_scale=kl_scale,\n",
        "            lambda_l1=lambda_l1,\n",
        "            lambda_fused=lambda_fused,\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            patience_reduce_lr=patience_reduce_lr,\n",
        "            patience_early_stop=patience_early_stop,\n",
        "            min_lr=min_lr,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        # Compute validation RMSE for evaluation\n",
        "        y_pred = model.predict(x_val, batch_size=batch_size, verbose=0).flatten()\n",
        "        y_true = y_val.flatten()\n",
        "        val_rmse = float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "        print(f\"  Validation RMSE: {val_rmse:.5f}\")\n",
        "        return val_rmse\n",
        "\n",
        "    # Create and run the Optuna study\n",
        "    study = optuna.create_study(direction=\"minimize\")\n",
        "    study.optimize(\n",
        "        objective,\n",
        "        timeout=timeout_time,\n",
        "        show_progress_bar=True,\n",
        "        catch=(Exception,)  # Ensures optimization continues on error\n",
        "    )\n",
        "\n",
        "    best_val_rmse = study.best_value\n",
        "    best_params = study.best_trial.params\n",
        "\n",
        "    return study, best_val_rmse, best_params\n",
        "\n",
        "def test_bcnn(\n",
        "    model: tf.keras.Model,\n",
        "    x_test_data: np.ndarray,\n",
        "    y_test_data: np.ndarray,\n",
        "    batch_size: int,\n",
        "    num_monte_carlo: int\n",
        ") -> tuple:\n",
        "    \"\"\"\n",
        "    Evaluate a trained Bayesian CNN model on a hold-out test set using Monte Carlo sampling.\n",
        "\n",
        "    Parameters:\n",
        "        model            : Trained BCNN model\n",
        "        x_test_data      : Test features\n",
        "        y_test_data      : Test targets\n",
        "        batch_size       : Batch size used during prediction\n",
        "        num_monte_carlo  : Number of forward passes for uncertainty estimation\n",
        "\n",
        "    Returns:\n",
        "        test_rmsep              : Root mean squared error of prediction\n",
        "        test_r2                 : R² score\n",
        "        test_practical_accuracy: % predictions within ±20% of observed values\n",
        "        df_predictions          : DataFrame containing MC predictions, observed and averaged predictions\n",
        "    \"\"\"\n",
        "\n",
        "    # === Monte Carlo Predictions ===\n",
        "    # Perform multiple stochastic forward passes\n",
        "    mc_preds = []\n",
        "    for i in range(num_monte_carlo):\n",
        "        preds = model.predict(\n",
        "            x_test_data,\n",
        "            batch_size=batch_size,\n",
        "            verbose=0\n",
        "        )\n",
        "        mc_preds.append(preds.flatten())\n",
        "\n",
        "    # Stack into [n_samples, num_monte_carlo] matrix\n",
        "    mc_preds = np.stack(mc_preds, axis=1)\n",
        "\n",
        "    # === Create Prediction DataFrame ===\n",
        "    df_predictions = pd.DataFrame(\n",
        "        mc_preds,\n",
        "        columns=[f\"mc_pass_{i+1}\" for i in range(num_monte_carlo)]\n",
        "    )\n",
        "\n",
        "    # === Evaluation ===\n",
        "    y_true = y_test_data.flatten()\n",
        "    y_pred = df_predictions.mean(axis=1).values\n",
        "\n",
        "    # Add to DataFrame for inspection\n",
        "    df_predictions[\"observed\"] = y_true\n",
        "    df_predictions[\"predicted\"] = y_pred\n",
        "\n",
        "    # Compute metrics\n",
        "    test_rmsep = float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "    test_r2 = float(r2_score(y_true, y_pred))\n",
        "    pct_error = np.abs(y_pred - y_true) / np.abs(y_true)\n",
        "    test_practical_accuracy = float((pct_error <= 0.2).mean() * 100.0)\n",
        "\n",
        "    # === Reporting ===\n",
        "    print(f\"Test RMSEP: {test_rmsep:.4f}\")\n",
        "    print(f\"Test R²: {test_r2:.4f}\")\n",
        "    print(f\"Practical accuracy (±20%): {test_practical_accuracy:.1f}%\")\n",
        "\n",
        "    # ---------- Parity Plot ----------\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(y_true, y_pred, alpha=0.7, label=\"Test Data\")\n",
        "    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], \"k--\", lw=2, label=\"Ideal\")\n",
        "    plt.xlabel(\"Observed\")\n",
        "    plt.ylabel(\"Predicted\")\n",
        "    plt.title(\"Observed vs. Predicted on Test Set (BCNN)\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    return test_rmsep, test_r2, test_practical_accuracy, df_predictions\n",
        "\n",
        "@tfd.RegisterKL(tfd.Independent, FusedLassoPrior)\n",
        "def kl_independent_fused_lasso(q, p, **kwargs):\n",
        "    \"\"\"\n",
        "    Custom KL divergence between an Independent distribution (posterior)\n",
        "    and a FusedLassoPrior (prior). Used in variational inference for Bayesian layers.\n",
        "\n",
        "    Parameters:\n",
        "        q : tfd.Independent\n",
        "            The approximate posterior distribution (usually Normal wrapped with Independent).\n",
        "        p : FusedLassoPrior\n",
        "            The custom fused lasso prior distribution.\n",
        "        **kwargs : dict\n",
        "            Extra keyword arguments required by TFP's KL interface (unused here).\n",
        "\n",
        "    Returns:\n",
        "        kl_divergence : tf.Tensor\n",
        "            A scalar tensor representing the KL divergence approximation.\n",
        "    \"\"\"\n",
        "    # Evaluate the log-probability of the posterior mean under the prior\n",
        "    # This acts as a surrogate for the full KL divergence\n",
        "    return -p.log_prob(q.mean())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1754484178288
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### _Parameters_"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "DF                              = df\n",
        "RANDOM_STATE                    = 27\n",
        "\n",
        "N_SUBSET                        = 22960\n",
        "VALIDATION_SIZE                 = 0.1\n",
        "TEST_VARIETY                    = \"TestVariety\"\n",
        "TEST_SEASON                     = 2025\n",
        "\n",
        "PATIENCE_CALLBACK_REDUCE_LR     = 25\n",
        "PATIENCE_CALLBACK_EARLY_STOP    = 50\n",
        "MIN_LR                          = 1e-6\n",
        "\n",
        "KERNEL_SIZE_RANGE               = (3, 1025)\n",
        "BATCH_SIZE_OPTIONS              = [32, 64, 128, 256, 512, 1024]     \n",
        "DROPOUT_RANGE                   = (0.01, 0.4)\n",
        "L2_RANGE                        = (1e-6, 1e-2)\n",
        "KL_SCALE_RANGE                  = (1e-6, 1e-2)\n",
        "LASSO_RANGE                     = (1e-4, 10)\n",
        "TIMEOUT_TIME                    = 60 * 60 * 72\n",
        "\n",
        "TRAIN_EPOCHS                    = 250\n",
        "TEST_EPOCHS                     = 1000\n",
        "NUM_MONTE_CARLO                 = 100"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1754484181310
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Split into train and test sets ===\n",
        "df_train_all, df_test_variety, df_test_season = split_train_test(\n",
        "    df,\n",
        "    test_variety=TEST_VARIETY,\n",
        "    test_season=TEST_SEASON,\n",
        ")\n",
        "\n",
        "# === Take subset ===\n",
        "df_subset = take_subset(\n",
        "    df_train_all, \n",
        "    n_subset=N_SUBSET, \n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "# === Make train/validation split ===\n",
        "df_train, df_val = create_train_val_split(\n",
        "    df=df_subset,\n",
        "    validation_size=VALIDATION_SIZE,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "# === Convert to x and y arrays ===\n",
        "x_train_all, y_train_all = split_x_y(\n",
        "    df_train_all,\n",
        ")\n",
        "x_train, y_train = split_x_y(\n",
        "    df_train,\n",
        ")\n",
        "x_val, y_val = split_x_y(\n",
        "    df_val,\n",
        ")\n",
        "x_test_variety, y_test_variety = split_x_y(\n",
        "    df_test_variety,\n",
        ")\n",
        "x_test_season, y_test_season = split_x_y(\n",
        "    df_test_season,\n",
        ")\n",
        "\n",
        "# === Search the best hyperparameters ===\n",
        "study, best_val_rmse, best_params = perform_optuna_hyperparameter_optimization(\n",
        "    x_train=x_train,\n",
        "    y_train=y_train,\n",
        "    x_val=x_val,\n",
        "    y_val=y_val,\n",
        "    input_shape=x_train.shape[1],\n",
        "    random_state=RANDOM_STATE,\n",
        "    epochs=TRAIN_EPOCHS,\n",
        "    patience_reduce_lr=PATIENCE_CALLBACK_REDUCE_LR,\n",
        "    patience_early_stop=PATIENCE_CALLBACK_EARLY_STOP,\n",
        "    min_lr=MIN_LR,\n",
        "    kernel_size_range=KERNEL_SIZE_RANGE,\n",
        "    batch_size_list=BATCH_SIZE_OPTIONS,\n",
        "    dropout_range=DROPOUT_RANGE,\n",
        "    l2_range=L2_RANGE,\n",
        "    kl_range=KL_SCALE_RANGE,\n",
        "    lasso_range=LASSO_RANGE,\n",
        "    timeout_time=TIMEOUT_TIME\n",
        ")\n",
        "\n",
        "print(\"Best hyperparameters found:\")\n",
        "for k, v in best_params.items():\n",
        "    print(f\"  {k:<15} = {v}\")\n",
        "\n",
        "# === Train the best model ===    \n",
        "bcnn_trained, _ = train_bcnn(\n",
        "    x_train=x_train_all,\n",
        "    y_train=y_train_all,\n",
        "    x_val=None,\n",
        "    y_val=None,\n",
        "    input_shape=x_train_all.shape[1],\n",
        "    kernel_size=best_params[\"kernel_size\"],\n",
        "    dropout_rate=best_params[\"dropout_rate\"],\n",
        "    l2_strength=best_params[\"l2_strength\"],\n",
        "    random_state=RANDOM_STATE,\n",
        "    kl_scale=best_params[\"kl_scale\"],\n",
        "    batch_size=best_params[\"batch_size\"],\n",
        "    epochs=TEST_EPOCHS,\n",
        "    patience_reduce_lr=PATIENCE_CALLBACK_REDUCE_LR,\n",
        "    patience_early_stop=PATIENCE_CALLBACK_EARLY_STOP,\n",
        "    min_lr=MIN_LR,\n",
        "    lambda_l1=best_params[\"lambda_l1\"],\n",
        "    lambda_fused=best_params[\"lambda_fused\"]\n",
        ")\n",
        "\n",
        "\n",
        "# === Test on the test sets ===\n",
        "rmsep_variety, r2_variety, acc_variety, df_predictions_variety = test_bcnn(\n",
        "    model=bcnn_trained,\n",
        "    x_test_data=x_test_variety,\n",
        "    y_test_data=y_test_variety,\n",
        "    batch_size=best_params[\"batch_size\"],\n",
        "    num_monte_carlo=NUM_MONTE_CARLO\n",
        ")\n",
        "print(f\"VARIETY: RMSEP={rmsep_variety:.3f}, R2={r2_variety:.3f}, ACC(±20%)={acc_variety:.1f}%s\")\n",
        "\n",
        "rmsep_season, r2_season, acc_season, df_predictions_season = test_bcnn(\n",
        "    model=bcnn_trained,\n",
        "    x_test_data=x_test_season,\n",
        "    y_test_data=y_test_season,\n",
        "    batch_size=best_params[\"batch_size\"],\n",
        "    num_monte_carlo=NUM_MONTE_CARLO\n",
        ")\n",
        "print(f\"SEASON:  RMSEP={rmsep_season:.3f}, R2={r2_season:.3f}, ACC(±20%)={acc_season:.1f}%s\")\n",
        "\n",
        "# Construct the results\n",
        "results = {\n",
        "    \"Test Set\": [\"VARIETY\", \"SEASON\"],\n",
        "    \"RMSEP\": [rmsep_variety, rmsep_season],\n",
        "    \"R2\": [r2_variety, r2_season],\n",
        "    \"Accuracy (±20%)\": [acc_variety, acc_season]\n",
        "}\n",
        "\n",
        "# Create a DataFrame\n",
        "df_results = pd.DataFrame(results)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1754485393961
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### _Inference Time Analysis_"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_inference_sample_set(\n",
        "    df_variety: pd.DataFrame,\n",
        "    df_season: pd.DataFrame,\n",
        "    random_state: int,\n",
        "    sample_size: int = 1000\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Combine two test sets (variety and season), sample rows randomly, and return X and y arrays.\n",
        "\n",
        "    Parameters:\n",
        "        df_variety   : DataFrame for variety-based test set\n",
        "        df_season    : DataFrame for season-based test set\n",
        "        random_state : Random seed for reproducibility\n",
        "        sample_size  : Number of rows to sample from combined test set\n",
        "\n",
        "    Returns:\n",
        "        x_sample : NumPy array of shape (sample_size, n_features) with spectral features\n",
        "        y_sample : NumPy array of shape (sample_size,) with corresponding Brix values\n",
        "    \"\"\"\n",
        "    # Combine the two test sets\n",
        "    df_combined = pd.concat([df_variety, df_season], axis=0)\n",
        "\n",
        "    # Randomly sample rows from the combined test set\n",
        "    df_sample = df_combined.sample(\n",
        "        n=sample_size,\n",
        "        random_state=random_state\n",
        "    )\n",
        "\n",
        "    # Split into X and y arrays\n",
        "    x_sample, y_sample = split_x_y(df_sample)\n",
        "\n",
        "    return x_sample, y_sample\n",
        "\n",
        "def test_bcnn_inference_time(\n",
        "    model: tf.keras.Model,\n",
        "    x_test: np.ndarray,\n",
        "    num_monte_carlo: int\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Measure average one-by-one inference time of a Bayesian CNN model\n",
        "    using Monte Carlo sampling over all test samples.\n",
        "\n",
        "    Parameters:\n",
        "        model            : Trained Bayesian CNN model\n",
        "        x_test           : Test feature matrix\n",
        "        num_monte_carlo  : Number of Monte Carlo forward passes per sample\n",
        "\n",
        "    Returns:\n",
        "        avg_inference_time_ms : Average inference time per sample in milliseconds\n",
        "    \"\"\"\n",
        "    times = []\n",
        "\n",
        "    for x in x_test:\n",
        "        x_input = np.expand_dims(x, axis=0)  # shape: (1, n_features)\n",
        "        start = time.time()\n",
        "\n",
        "        for _ in range(num_monte_carlo):\n",
        "            _ = model(x_input, training=True).numpy()\n",
        "\n",
        "        end = time.time()\n",
        "        times.append(end - start)\n",
        "\n",
        "    avg_inference_time_ms = np.mean(times) * 1000  # Convert to ms\n",
        "\n",
        "    print(f\"Average BCNN inference time: {avg_inference_time_ms:.3f} ms/sample \"\n",
        "          f\"(with {num_monte_carlo} MC passes)\")\n",
        "\n",
        "    return avg_inference_time_ms\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1754485438248
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Create sample set for inference time measurement ===\n",
        "x_inference_time, y_inference_time = get_inference_sample_set(\n",
        "    df_test_variety,\n",
        "    df_test_season,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "# === Compute the average inference time ===\n",
        "bcnn_time_ms = test_bcnn_inference_time(\n",
        "    model=bcnn_trained,\n",
        "    x_test=x_inference_time,\n",
        "    num_monte_carlo=NUM_MONTE_CARLO\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1754485480685
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "tf",
      "language": "python",
      "display_name": "Python (tf)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.15",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "tf"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}