{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### _Setup_"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset memory\n",
        "%reset -f"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Install correct package versions\n",
        "!pip install \"tensorflow[and-cuda]\"\n",
        "!pip uninstall numpy pandas -y\n",
        "!pip install \"numpy<2.0\" pandas --upgrade --no-cache-dir"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1754482040992
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Packages\n",
        "from typing import Union, List, Tuple, Dict, Any\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import KFold, train_test_split, StratifiedShuffleSplit\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers, initializers, optimizers, callbacks\n",
        "import optuna\n",
        "import matplotlib.pyplot as plt"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1754482127728
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU check\n",
        "print(\"Available GPUs:\", tf.config.list_physical_devices('GPU'))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1754482133530
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data\n",
        "df = pd.read_csv('data.csv')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1754482160481
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### _Functions_"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_col_index_of_spectra(\n",
        "    df: pd.DataFrame\n",
        ") -> int:\n",
        "    \"\"\"\n",
        "    Find the column index where spectral data starts.\n",
        "\n",
        "    Assumes spectral column names can be converted to float (e.g., \"730.5\", \"731.0\").\n",
        "\n",
        "    Parameters:\n",
        "        df : Input DataFrame\n",
        "\n",
        "    Returns:\n",
        "        Index of the first spectral column, or -1 if not found.\n",
        "    \"\"\"\n",
        "    for idx, col in enumerate(df.columns):\n",
        "        try:\n",
        "            float(col)\n",
        "            return idx\n",
        "        except (ValueError, TypeError):\n",
        "            continue\n",
        "    return -1\n",
        "\n",
        "def split_train_test(\n",
        "    df: pd.DataFrame,\n",
        "    test_variety: str,\n",
        "    test_season: int       \n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Split a DataFrame into one training set and two test sets:\n",
        "\n",
        "    - Variety test set: Variety == test_variety AND Year == 2024\n",
        "    - Season test set : Year == test_season \n",
        "\n",
        "    The training set excludes all rows that belong to any of the test sets.\n",
        "    The season test set only includes varieties that are present in the training set.\n",
        "\n",
        "    Parameters:\n",
        "        df           : Full pandas DataFrame\n",
        "        test_variety : Variety used for the test set\n",
        "        test_season  : Year used for the season test\n",
        "\n",
        "    Returns:\n",
        "        df_train        : Training set\n",
        "        df_test_variety : Test set for specified variety and 2024\n",
        "        df_test_season  : Test set for specified season (filtered by train varieties)\n",
        "    \"\"\"\n",
        "\n",
        "    # Select test set for the specified variety in year 2024\n",
        "    df_test_variety = df[\n",
        "        (df[\"Variety\"] == test_variety) &\n",
        "        (df[\"Scan Date Year\"] == 2024)\n",
        "    ]\n",
        "\n",
        "    # Select test set for the specified season (regardless of variety)\n",
        "    df_test_season = df[\n",
        "        df[\"Scan Date Year\"] == test_season\n",
        "    ]\n",
        "\n",
        "    # Select training set (exclude test variety and test season)\n",
        "    df_train = df[\n",
        "        (df[\"Variety\"] != test_variety) &\n",
        "        (df[\"Scan Date Year\"] != test_season)\n",
        "    ]\n",
        "\n",
        "    # Filter season test set to only include varieties present in training set\n",
        "    train_varieties = df_train[\"Variety\"].unique()\n",
        "    df_test_season = df_test_season[\n",
        "        df_test_season[\"Variety\"].isin(train_varieties)\n",
        "    ]\n",
        "\n",
        "    return df_train, df_test_variety, df_test_season\n",
        "\n",
        "def take_subset(\n",
        "    df: pd.DataFrame, \n",
        "    n_subset: int,\n",
        "    random_state: int\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Return a stratified subset of the DataFrame based on 10 Brix bins.\n",
        "\n",
        "    If n_subset >= len(df), the original DataFrame is returned.\n",
        "\n",
        "    Parameters:\n",
        "        df       : Input DataFrame with 'Brix (Position)' column\n",
        "        n_subset : Desired subset size\n",
        "        random_state : Random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "        Subset of df with stratification over 10 quantile bins of Brix\n",
        "    \"\"\"\n",
        "    # If requested subset size exceeds full dataset, return a copy of the full DataFrame\n",
        "    if n_subset >= len(df):\n",
        "        return df.copy()\n",
        "\n",
        "    # Bin the Brix values into 10 quantile-based bins for stratification\n",
        "    binned = pd.qcut(df[\"Brix (Position)\"], q=10, labels=False, duplicates='drop')\n",
        "\n",
        "    # Initialize stratified sampler\n",
        "    splitter = StratifiedShuffleSplit(\n",
        "        n_splits=1,\n",
        "        train_size=n_subset,\n",
        "        random_state=random_state\n",
        "    )\n",
        "\n",
        "    # Perform stratified split and extract subset indices\n",
        "    idx_subset, _ = next(splitter.split(df, binned))\n",
        "\n",
        "    # Return the stratified subset as a new DataFrame with reset index\n",
        "    return df.iloc[idx_subset].reset_index(drop=True)\n",
        "\n",
        "def create_train_val_split(\n",
        "    df: pd.DataFrame,\n",
        "    validation_size: float,\n",
        "    random_state: int\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Split a DataFrame into train and validation sets using stratified sampling\n",
        "    based on 10 quantile bins of the 'Brix (Position)' column.\n",
        "\n",
        "    Parameters:\n",
        "        df              : Input DataFrame\n",
        "        validation_size : Proportion of validation samples (0 < float < 1)\n",
        "        random_state    : Seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "        df_train, df_val : Stratified training and validation DataFrames\n",
        "    \"\"\"\n",
        "    # Bin the Brix values into 10 quantile-based bins for stratified splitting\n",
        "    binned = pd.qcut(df[\"Brix (Position)\"], q=10, labels=False, duplicates=\"drop\")\n",
        "\n",
        "    # Perform stratified train/validation split based on the binned Brix values\n",
        "    df_train, df_val = train_test_split(\n",
        "        df,\n",
        "        test_size=validation_size,\n",
        "        random_state=random_state,\n",
        "        stratify=binned\n",
        "    )\n",
        "\n",
        "    # Return splits with reset indices\n",
        "    return df_train.reset_index(drop=True), df_val.reset_index(drop=True)\n",
        "\n",
        "def split_x_y(\n",
        "    df: pd.DataFrame,\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Split a DataFrame into X (spectral features) and y (target) arrays.\n",
        "    Assumes find_col_index_of_spectra() is defined globally and returns the index\n",
        "    where spectral data starts.\n",
        "\n",
        "    Parameters:\n",
        "        df : Input DataFrame containing both metadata and spectral data.\n",
        "\n",
        "    Returns:\n",
        "        x : NumPy array of shape (n_samples, n_spectral_features)\n",
        "        y : NumPy array of shape (n_samples, 1) containing Brix values\n",
        "    \"\"\"\n",
        "    # Identify spectral columns (those that can be cast to float, e.g. wavelengths)\n",
        "    spectra_cols = list(df.columns[find_col_index_of_spectra(df):])\n",
        "\n",
        "    # Define the target column\n",
        "    target_cols = ['Brix (Position)']\n",
        "\n",
        "    # Extract feature and target arrays\n",
        "    x = df[spectra_cols].values\n",
        "    y = df[target_cols].values\n",
        "\n",
        "    return x, y\n",
        "\n",
        "def rmse_loss(\n",
        "    y_true, \n",
        "    y_pred\n",
        "):\n",
        "    \"\"\"\n",
        "    Compute the Root Mean Squared Error (RMSE) as a loss function.\n",
        "\n",
        "    Parameters:\n",
        "        y_true : Tensor of true target values\n",
        "        y_pred : Tensor of predicted values\n",
        "\n",
        "    Returns:\n",
        "        RMSE as a scalar Tensor\n",
        "    \"\"\"\n",
        "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))  \n",
        "\n",
        "def rmse_metric(\n",
        "    y_true, \n",
        "    y_pred\n",
        "):\n",
        "    \"\"\"\n",
        "    Compute the Root Mean Squared Error (RMSE) as a performance metric.\n",
        "\n",
        "    Parameters:\n",
        "        y_true : Tensor of true target values\n",
        "        y_pred : Tensor of predicted values\n",
        "\n",
        "    Returns:\n",
        "        RMSE as a scalar Tensor\n",
        "    \"\"\"\n",
        "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))  \n",
        "\n",
        "def cnn_model(\n",
        "    input_shape: int,\n",
        "    kernel_size: int,\n",
        "    dropout_rate: float,\n",
        "    l2_strength: float,\n",
        "    learning_rate: float,\n",
        "    random_state: int\n",
        ") -> tf.keras.Model:\n",
        "    \"\"\"\n",
        "    Build and compile a 1D Convolutional Neural Network for regression tasks.\n",
        "\n",
        "    Parameters:\n",
        "        input_shape    : Number of input features (spectral length).\n",
        "        kernel_size    : Size of the 1D convolutional kernel.\n",
        "        dropout_rate   : Dropout rate used after each dense block.\n",
        "        l2_strength    : L2 regularization strength for kernel weights.\n",
        "        learning_rate  : Learning rate for the Adam optimizer.\n",
        "        random_state   : Random seed for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "        model : Compiled Keras model ready for training.\n",
        "    \"\"\"\n",
        "    # Define kernel regularizer and initializer\n",
        "    kernel_reg  = regularizers.l2(l2_strength)\n",
        "    kernel_init = initializers.HeNormal(seed=random_state)\n",
        "\n",
        "    # Build model architecture\n",
        "    model = models.Sequential([\n",
        "        tf.keras.Input(shape=(input_shape,)),              # Input layer\n",
        "        layers.Reshape((input_shape, 1)),                  # Reshape to (timesteps, 1) for Conv1D\n",
        "        layers.Conv1D(                                     # Convolutional layer\n",
        "            filters=1,\n",
        "            kernel_size=kernel_size,\n",
        "            padding=\"same\",\n",
        "            activation=\"elu\",\n",
        "            kernel_initializer=kernel_init,\n",
        "            kernel_regularizer=kernel_reg\n",
        "        ),\n",
        "        layers.Dropout(dropout_rate),                      # Regularization\n",
        "        layers.Flatten(),                                  # Flatten before dense layers\n",
        "        layers.Dense(36, activation=\"elu\", kernel_initializer=kernel_init, kernel_regularizer=kernel_reg),\n",
        "        layers.Dropout(dropout_rate),\n",
        "        layers.Dense(18, activation=\"elu\", kernel_initializer=kernel_init, kernel_regularizer=kernel_reg),\n",
        "        layers.Dropout(dropout_rate),\n",
        "        layers.Dense(12, activation=\"elu\", kernel_initializer=kernel_init, kernel_regularizer=kernel_reg),\n",
        "        layers.Dense(1, activation=\"linear\", kernel_initializer=kernel_init, kernel_regularizer=kernel_reg)  # Output\n",
        "    ])\n",
        "\n",
        "    # Compile the model with custom RMSE loss and metric\n",
        "    model.compile(\n",
        "        optimizer=optimizers.Adam(learning_rate=learning_rate),\n",
        "        loss=rmse_loss,\n",
        "        metrics=[rmse_metric]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "def perform_optuna_hyperparameter_optimization(\n",
        "    x_train_data: np.ndarray,\n",
        "    y_train_data: np.ndarray,\n",
        "    x_val_data: np.ndarray,\n",
        "    y_val_data: np.ndarray,\n",
        "    batch_size_range: List[int],\n",
        "    l2_strength_range: Tuple[float, float],\n",
        "    kernel_size_range: Tuple[int, int],\n",
        "    dropout_rate_range: Tuple[float, float],\n",
        "    random_state: int,\n",
        "    patience_callback_reduce_lr: int,\n",
        "    patience_callback_early_stopping: int,\n",
        "    epochs: int,\n",
        "    min_lr: float,\n",
        "    timeout: int\n",
        ") -> Tuple[optuna.study.Study, float, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Optimize CNN hyperparameters using a fixed validation set with Optuna.\n",
        "\n",
        "    Parameters:\n",
        "        x_train_data, y_train_data : Arrays of training features and targets\n",
        "        x_val_data, y_val_data     : Arrays of validation features and targets\n",
        "        batch_size_range           : List of batch sizes to try\n",
        "        l2_strength_range          : Tuple defining (min, max) for L2 regularization\n",
        "        kernel_size_range          : Tuple defining (min, max) for Conv1D kernel size\n",
        "        dropout_rate_range         : Tuple defining (min, max) for dropout rate\n",
        "        random_state               : Random seed for reproducibility\n",
        "        patience_callback_*        : Patience for LR scheduler and early stopping\n",
        "        epochs                     : Max number of training epochs\n",
        "        min_lr                     : Minimum learning rate for LR scheduler\n",
        "        timeout                    : Max Optuna runtime (seconds)\n",
        "\n",
        "    Returns:\n",
        "        study       : Optuna Study object\n",
        "        best_value  : Best validation RMSE found\n",
        "        best_params : Best-performing hyperparameter set\n",
        "    \"\"\"\n",
        "\n",
        "    def objective(trial):\n",
        "        # Suggest hyperparameters\n",
        "        batch_size   = trial.suggest_categorical(\"batch_size\", batch_size_range)\n",
        "        l2_strength  = trial.suggest_float(\"l2_strength\", *l2_strength_range)\n",
        "        kernel_size  = trial.suggest_int(\"kernel_size\", *kernel_size_range, step=2)\n",
        "        dropout_rate = trial.suggest_float(\"dropout_rate\", *dropout_rate_range)\n",
        "\n",
        "        # Adjust learning rate proportionally to batch size\n",
        "        learning_rate = 0.01 * (batch_size / 256)\n",
        "\n",
        "        # Print trial configuration\n",
        "        print(f\"\\n[Trial {trial.number}] Testing hyperparameters:\")\n",
        "        print(f\"  batch_size   = {batch_size}\")\n",
        "        print(f\"  l2_strength  = {l2_strength:.2e}\")\n",
        "        print(f\"  kernel_size  = {kernel_size}\")\n",
        "        print(f\"  dropout_rate = {dropout_rate:.2f}\")\n",
        "\n",
        "        # Build CNN model with trial parameters\n",
        "        model = cnn_model(\n",
        "            input_shape=x_train_data.shape[1],\n",
        "            kernel_size=kernel_size,\n",
        "            dropout_rate=dropout_rate,\n",
        "            l2_strength=l2_strength,\n",
        "            learning_rate=learning_rate,\n",
        "            random_state=random_state\n",
        "        )\n",
        "\n",
        "        # Define training callbacks\n",
        "        cb = [\n",
        "            callbacks.ReduceLROnPlateau(\n",
        "                monitor=\"val_loss\",\n",
        "                factor=0.5,\n",
        "                patience=patience_callback_reduce_lr,\n",
        "                min_lr=min_lr,\n",
        "                verbose=0\n",
        "            ),\n",
        "            callbacks.EarlyStopping(\n",
        "                monitor=\"val_loss\",\n",
        "                patience=patience_callback_early_stopping,\n",
        "                restore_best_weights=True,\n",
        "                verbose=0\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        # Train the model\n",
        "        model.fit(\n",
        "            x_train_data, y_train_data,\n",
        "            validation_data=(x_val_data, y_val_data),\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            callbacks=cb,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        # Evaluate model on validation set\n",
        "        preds = model.predict(x_val_data, batch_size=batch_size, verbose=0)\n",
        "        rmse = np.sqrt(mean_squared_error(y_val_data, preds))\n",
        "\n",
        "        print(f\"  → Val RMSE: {rmse:.4f}\")\n",
        "        return float(rmse)\n",
        "\n",
        "    # Create and run Optuna study\n",
        "    study = optuna.create_study(direction=\"minimize\")\n",
        "    study.optimize(objective, timeout=timeout)\n",
        "\n",
        "    return study, float(study.best_value), study.best_params\n",
        "\n",
        "def train_cnn_model(\n",
        "    x_train_data: np.ndarray,\n",
        "    y_train_data: np.ndarray,\n",
        "    best_params: Dict[str, Any],\n",
        "    random_state: int,\n",
        "    patience_callback_reduce_lr: int,\n",
        "    patience_callback_early_stopping: int,\n",
        "    epochs: int,\n",
        "    min_lr: float\n",
        ") -> tf.keras.Model:\n",
        "    \"\"\"\n",
        "    Train a CNN model using the full training set and Optuna-derived hyperparameters.\n",
        "\n",
        "    Parameters:\n",
        "        x_train_data : Training features\n",
        "        y_train_data : Training targets\n",
        "        best_params  : Dictionary with optimal hyperparameters from Optuna\n",
        "        random_state : Seed for reproducibility\n",
        "        patience_callback_reduce_lr     : Patience for learning rate reduction\n",
        "        patience_callback_early_stopping: Patience for early stopping\n",
        "        epochs       : Maximum number of training epochs\n",
        "        min_lr       : Minimum learning rate allowed by the scheduler\n",
        "\n",
        "    Returns:\n",
        "        model : Trained Keras CNN model\n",
        "    \"\"\"\n",
        "\n",
        "    # === Unpack hyperparameters ===\n",
        "    batch_size   = best_params[\"batch_size\"]\n",
        "    l2_strength  = best_params[\"l2_strength\"]\n",
        "    kernel_size  = best_params[\"kernel_size\"]\n",
        "    dropout_rate = best_params[\"dropout_rate\"]\n",
        "\n",
        "    # Learning rate scaled with batch size (same formula used in Optuna tuning)\n",
        "    learning_rate = 0.01 * (batch_size / 256)\n",
        "\n",
        "    # === Build CNN model with chosen hyperparameters ===\n",
        "    model = cnn_model(\n",
        "        input_shape=x_train_data.shape[1],\n",
        "        kernel_size=kernel_size,\n",
        "        dropout_rate=dropout_rate,\n",
        "        l2_strength=l2_strength,\n",
        "        learning_rate=learning_rate,\n",
        "        random_state=random_state\n",
        "    )\n",
        "\n",
        "    # === Configure callbacks ===\n",
        "    cb = [\n",
        "        callbacks.ReduceLROnPlateau(\n",
        "            monitor=\"loss\",                     # Monitors training loss\n",
        "            factor=0.5,                         # Halve learning rate when triggered\n",
        "            patience=patience_callback_reduce_lr,\n",
        "            min_lr=min_lr,\n",
        "            verbose=0\n",
        "        ),\n",
        "        callbacks.EarlyStopping(\n",
        "            monitor=\"loss\",                     # Stops early if training loss plateaus\n",
        "            patience=patience_callback_early_stopping,\n",
        "            restore_best_weights=True,\n",
        "            verbose=0\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # === Train model ===\n",
        "    model.fit(\n",
        "        x_train_data,\n",
        "        y_train_data,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        callbacks=cb,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "def test_cnn_model(\n",
        "    model: tf.keras.Model,\n",
        "    x_test_data: np.ndarray,\n",
        "    y_test_data: np.ndarray,\n",
        "    batch_size: int\n",
        ") -> Tuple[float, float, float, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Evaluate a trained CNN model on a hold-out test set.\n",
        "\n",
        "    Parameters:\n",
        "        model       : Trained Keras model\n",
        "        x_test_data : Test feature matrix\n",
        "        y_test_data : Test target vector\n",
        "        batch_size  : Batch size for prediction\n",
        "\n",
        "    Returns:\n",
        "        test_rmsep              : Root mean squared error of prediction\n",
        "        test_r2                 : Coefficient of determination (R² score)\n",
        "        test_practical_accuracy : Percentage of predictions within ±20% of actual value\n",
        "        df_results              : DataFrame with columns ['predicted', 'observed']\n",
        "    \"\"\"\n",
        "\n",
        "    # === Run model inference ===\n",
        "    y_pred = model.predict(x_test_data, batch_size=batch_size, verbose=0).flatten()\n",
        "    y_true = y_test_data.flatten()\n",
        "\n",
        "    # === Compute evaluation metrics ===\n",
        "    test_rmsep = float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "    test_r2 = float(r2_score(y_true, y_pred))\n",
        "    pct_error = np.abs(y_pred - y_true) / np.abs(y_true)\n",
        "    test_practical_accuracy = float((pct_error <= 0.2).mean() * 100.0)\n",
        "\n",
        "    # === Print evaluation summary ===\n",
        "    print(f\"Test RMSEP: {test_rmsep:.4f}\")\n",
        "    print(f\"Test R²: {test_r2:.4f}\")\n",
        "    print(f\"Practical accuracy (±20%): {test_practical_accuracy:.1f}%\")\n",
        "\n",
        "    # === Plot parity plot ===\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(y_true, y_pred, alpha=0.7, label=\"Test Data\")\n",
        "    plt.plot(\n",
        "        [y_true.min(), y_true.max()],\n",
        "        [y_true.min(), y_true.max()],\n",
        "        \"k--\", lw=2, label=\"Ideal\"\n",
        "    )\n",
        "    plt.xlabel(\"Observed\")\n",
        "    plt.ylabel(\"Predicted\")\n",
        "    plt.title(\"Observed vs. Predicted on Test Set (CNN)\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # === Create prediction result DataFrame ===\n",
        "    df_results = pd.DataFrame({\n",
        "        \"predicted\": y_pred,\n",
        "        \"observed\": y_true\n",
        "    })\n",
        "\n",
        "    return test_rmsep, test_r2, test_practical_accuracy, df_results\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1754482162323
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### _Parameters_"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DF              = df\n",
        "TEST_VARIETY    = \"TestVariety\"\n",
        "TEST_SEASON     = 2025\n",
        "\n",
        "RANDOM_STATE    = 27\n",
        "N_SUBSET        = 22892\n",
        "VALIDATION_SIZE = 0.1\n",
        "\n",
        "BATCH_SIZE_RANGE              = [32, 64, 128, 256, 512, 1024]\n",
        "L2_STRENGTH_RANGE             = (1e-6, 1e-2)\n",
        "KERNEL_SIZE_RANGE             = (3, 1025)\n",
        "DROPOUT_RATE_RANGE            = (0.01, 0.4)\n",
        "PATIENCE_CALLBACK_REDUCE_LR   = 25\n",
        "PATIENCE_CALLBACK_EARLY_STOP  = 50\n",
        "\n",
        "TRAIN_EPOCHS                  = 500\n",
        "TEST_EPOCHS                   = 1000\n",
        "MIN_LR                        = 1e-6\n",
        "TIMEOUT                       = 60 * 60 * 72"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1754482195282
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### _Run_"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Split into train and test sets ===\n",
        "df_train_all, df_test_variety, df_test_season = split_train_test(\n",
        "    df,\n",
        "    test_variety=TEST_VARIETY,\n",
        "    test_season=TEST_SEASON,\n",
        ")\n",
        "\n",
        "# === Take subset ===\n",
        "df_subset = take_subset(\n",
        "    df_train_all, \n",
        "    n_subset=N_SUBSET, \n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "# === Make train/validation split ===\n",
        "df_train, df_val = create_train_val_split(\n",
        "    df=df_subset,\n",
        "    validation_size=VALIDATION_SIZE,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "# === Convert to x and y arrays ===\n",
        "x_train_all, y_train_all = split_x_y(\n",
        "    df_train_all,\n",
        ")\n",
        "x_train, y_train = split_x_y(\n",
        "    df_train,\n",
        ")\n",
        "x_val, y_val = split_x_y(\n",
        "    df_val,\n",
        ")\n",
        "x_test_variety, y_test_variety = split_x_y(\n",
        "    df_test_variety,\n",
        ")\n",
        "x_test_season, y_test_season = split_x_y(\n",
        "    df_test_season,\n",
        ")\n",
        "\n",
        "print(\"begin optuna hyperparameter optimization\")\n",
        "# === CNN hyperparameter tuning ===\n",
        "study, best_rmse, best_params = perform_optuna_hyperparameter_optimization(\n",
        "    x_train_data=x_train,\n",
        "    y_train_data=y_train,\n",
        "    x_val_data=x_val,\n",
        "    y_val_data=y_val,\n",
        "    batch_size_range=BATCH_SIZE_RANGE,\n",
        "    l2_strength_range=L2_STRENGTH_RANGE,\n",
        "    kernel_size_range=KERNEL_SIZE_RANGE,\n",
        "    dropout_rate_range=DROPOUT_RATE_RANGE,\n",
        "    random_state=RANDOM_STATE,\n",
        "    patience_callback_reduce_lr=PATIENCE_CALLBACK_REDUCE_LR,\n",
        "    patience_callback_early_stopping=PATIENCE_CALLBACK_EARLY_STOP,\n",
        "    epochs=TRAIN_EPOCHS,\n",
        "    min_lr=MIN_LR,\n",
        "    timeout=TIMEOUT\n",
        ")\n",
        "\n",
        "print(f\"Best CNN RMSECV: {best_rmse:.4f}\")\n",
        "print(\"Best hyperparameters:\")\n",
        "for k, v in best_params.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "\n",
        "# === Model training on optimal paramters\n",
        "cnn_model = train_cnn_model(\n",
        "    x_train_data=x_train_all,\n",
        "    y_train_data=y_train_all,\n",
        "    best_params=best_params,\n",
        "    random_state=RANDOM_STATE,\n",
        "    patience_callback_reduce_lr=PATIENCE_CALLBACK_REDUCE_LR,\n",
        "    patience_callback_early_stopping=PATIENCE_CALLBACK_EARLY_STOP,\n",
        "    epochs=TEST_EPOCHS,\n",
        "    min_lr=MIN_LR\n",
        ")\n",
        "\n",
        "# === Model evaluation ===\n",
        "rmsep_variety, r2_variety, acc_variety, results_variety = test_cnn_model(\n",
        "    model=cnn_model,\n",
        "    x_test_data=x_test_variety,\n",
        "    y_test_data=y_test_variety,\n",
        "    batch_size=best_params[\"batch_size\"]\n",
        ")\n",
        "\n",
        "rmsep_season, r2_season, acc_season, results_season = test_cnn_model(\n",
        "    model=cnn_model,\n",
        "    x_test_data=x_test_season,\n",
        "    y_test_data=y_test_season,\n",
        "    batch_size=best_params[\"batch_size\"]\n",
        ")\n",
        "\n",
        "summary = pd.DataFrame({\n",
        "    \"Test Set\": [\"Variety\", \"Season\"],\n",
        "    \"RMSE\": [rmsep_variety, rmsep_season],\n",
        "    \"R²\": [r2_variety, r2_season],\n",
        "    \"% Within 20%\": [acc_variety, acc_season]\n",
        "})\n",
        "\n",
        "print(summary)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1754482285739
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### _Sensitivity Analysis_"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_inference_sample_set(\n",
        "    df_variety: pd.DataFrame,\n",
        "    df_season: pd.DataFrame,\n",
        "    random_state: int,\n",
        "    sample_size: int = 1000\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Combine two test sets (variety and season), sample rows randomly, and return X and y arrays.\n",
        "\n",
        "    Parameters:\n",
        "        df_variety   : DataFrame for variety-based test set\n",
        "        df_season    : DataFrame for season-based test set\n",
        "        random_state : Random seed for reproducibility\n",
        "        sample_size  : Number of rows to sample from combined test set\n",
        "\n",
        "    Returns:\n",
        "        x_sample : NumPy array of shape (sample_size, n_features) with spectral features\n",
        "        y_sample : NumPy array of shape (sample_size,) with corresponding Brix values\n",
        "    \"\"\"\n",
        "    # Combine the two test sets\n",
        "    df_combined = pd.concat([df_variety, df_season], axis=0)\n",
        "\n",
        "    # Randomly sample rows from the combined test set\n",
        "    df_sample = df_combined.sample(\n",
        "        n=sample_size,\n",
        "        random_state=random_state\n",
        "    )\n",
        "\n",
        "    # Split into X and y arrays\n",
        "    x_sample, y_sample = split_x_y(df_sample)\n",
        "\n",
        "    return x_sample, y_sample\n",
        "\n",
        "def test_cnn_inference_time(\n",
        "    model: tf.keras.Model,\n",
        "    x_test: np.ndarray\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Measure average one-by-one inference time of a CNN model in milliseconds.\n",
        "\n",
        "    Parameters:\n",
        "        model   : Trained Keras model\n",
        "        x_test  : Test feature matrix\n",
        "\n",
        "    Returns:\n",
        "        avg_inference_time_ms : Average inference time per sample in milliseconds\n",
        "    \"\"\"\n",
        "    times = []\n",
        "\n",
        "    for x in x_test:\n",
        "        x_input = np.expand_dims(x, axis=0)  # shape: (1, n_features)\n",
        "        start = time.time()\n",
        "        _ = model(x_input, training=False).numpy().flatten()[0]\n",
        "        end = time.time()\n",
        "        times.append(end - start)\n",
        "\n",
        "    avg_inference_time_ms = np.mean(times) * 1000\n",
        "    print(f\"Average inference time: {avg_inference_time_ms:.3f} ms/sample\")\n",
        "\n",
        "    return avg_inference_time_ms"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1754482344425
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Create sample set for inference time measurement ===\n",
        "x_inference_time, y_inference_time = get_inference_sample_set(\n",
        "    df_test_variety,\n",
        "    df_test_season,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "# === Compute the average inference time ===\n",
        "cnn_inference_time_ms = test_cnn_inference_time(\n",
        "    cnn_model, \n",
        "    x_inference_time\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1754482389508
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "tf",
      "language": "python",
      "display_name": "Python (tf)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.15",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "kernel_info": {
      "name": "tf"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}