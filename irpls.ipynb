{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### _Setup_"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset memory\n",
        "%reset -f"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1754473301156
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Union, Tuple\n",
        "from sklearn.cross_decomposition import PLSRegression\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import time\n",
        "from kneed import KneeLocator\n",
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1754473346012
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Data\n",
        "df = pd.read_csv('data.csv')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1754473381895
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### _Functions_"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_col_index_of_spectra(\n",
        "    df: pd.DataFrame\n",
        ") -> int:\n",
        "    \"\"\"\n",
        "    Find the column index where spectral data starts.\n",
        "\n",
        "    Assumes spectral column names can be converted to float (e.g., \"730.5\", \"731.0\").\n",
        "\n",
        "    Parameters:\n",
        "        df : Input DataFrame\n",
        "\n",
        "    Returns:\n",
        "        Index of the first spectral column, or -1 if not found.\n",
        "    \"\"\"\n",
        "    for idx, col in enumerate(df.columns):\n",
        "        try:\n",
        "            float(col)\n",
        "            return idx\n",
        "        except (ValueError, TypeError):\n",
        "            continue\n",
        "    return -1\n",
        "\n",
        "def split_train_test(\n",
        "    df: pd.DataFrame,\n",
        "    test_variety: str,\n",
        "    test_season: int       \n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Split a DataFrame into one training set and two test sets:\n",
        "\n",
        "    - Variety test set: Variety == test_variety AND Year == 2024\n",
        "    - Season test set : Year == test_season \n",
        "\n",
        "    The training set excludes all rows that belong to any of the test sets.\n",
        "    The season test set only includes varieties that are present in the training set.\n",
        "\n",
        "    Parameters:\n",
        "        df           : Full pandas DataFrame\n",
        "        test_variety : Variety used for the test set\n",
        "        test_season  : Year used for the season test\n",
        "\n",
        "    Returns:\n",
        "        df_train        : Training set\n",
        "        df_test_variety : Test set for specified variety and 2024\n",
        "        df_test_season  : Test set for specified season (filtered by train varieties)\n",
        "    \"\"\"\n",
        "\n",
        "    # Select test set for the specified variety in year 2024\n",
        "    df_test_variety = df[\n",
        "        (df[\"Variety\"] == test_variety) &\n",
        "        (df[\"Scan Date Year\"] == 2024)\n",
        "    ]\n",
        "\n",
        "    # Select test set for the specified season (regardless of variety)\n",
        "    df_test_season = df[\n",
        "        df[\"Scan Date Year\"] == test_season\n",
        "    ]\n",
        "\n",
        "    # Select training set (exclude test variety and test season)\n",
        "    df_train = df[\n",
        "        (df[\"Variety\"] != test_variety) &\n",
        "        (df[\"Scan Date Year\"] != test_season)\n",
        "    ]\n",
        "\n",
        "    # Filter season test set to only include varieties present in training set\n",
        "    train_varieties = df_train[\"Variety\"].unique()\n",
        "    df_test_season = df_test_season[\n",
        "        df_test_season[\"Variety\"].isin(train_varieties)\n",
        "    ]\n",
        "\n",
        "    return df_train, df_test_variety, df_test_season\n",
        "\n",
        "def split_x_y(\n",
        "    df: pd.DataFrame,\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Split train and test DataFrames into x (spectra features) and y (target) arrays.\n",
        "    Assumes find_col_index_of_spectra() is defined globally.\n",
        "\n",
        "    Parameters:\n",
        "        df_train: Training set DataFrame.\n",
        "        df_test : Test set DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        x_train: NumPy array of training features.\n",
        "        y_train: NumPy array of training targets.\n",
        "    \"\"\"\n",
        "    spectra_cols = list(df.columns[find_col_index_of_spectra(df):])\n",
        "    target_cols = ['Brix (Position)']\n",
        "\n",
        "    x = df[spectra_cols].values\n",
        "    y = df[target_cols].values\n",
        "\n",
        "    return (\n",
        "        x,\n",
        "        y\n",
        "    )\n",
        "\n",
        "def stratified_cv_splits(\n",
        "    x_train, \n",
        "    y_train, \n",
        "    n_splits, \n",
        "    random_state, \n",
        "    n_bins=10\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate stratified K-Fold splits for regression by binning y_train into quantiles.\n",
        "\n",
        "    Parameters:\n",
        "        x_train      : Feature matrix (NumPy array or DataFrame)\n",
        "        y_train      : Target values (NumPy array or Series)\n",
        "        n_splits     : Number of folds for StratifiedKFold\n",
        "        random_state : Random seed for reproducibility\n",
        "        n_bins       : Number of quantile bins to stratify target into (default: 10)\n",
        "\n",
        "    Returns:\n",
        "        Generator of (train_idx, val_idx) tuples\n",
        "    \"\"\"\n",
        "\n",
        "    # Bin the target into quantiles\n",
        "    y_binned = pd.qcut(np.ravel(y_train), q=n_bins, labels=False, duplicates='drop')\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
        "\n",
        "    for train_idx, val_idx in skf.split(x_train, y_binned):\n",
        "        yield train_idx, val_idx\n",
        "\n",
        "def model_irpls(\n",
        "    x_data: np.ndarray,\n",
        "    y_data: np.ndarray,\n",
        "    n_components: int,\n",
        "    max_iter: int,\n",
        "    tol: float,\n",
        "    c: float,\n",
        "    verbose: bool = False\n",
        ") -> tuple[PLSRegression, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Iteratively Reweighted PLS with per‐component deflation (irPLS).\n",
        "\n",
        "    This follows the MATLAB irpls.m structure: for each latent component,\n",
        "    we iteratively update a diagonal weight matrix D until its sum converges,\n",
        "    then deflate X and Y, and move on to the next component.\n",
        "\n",
        "    Parameters:\n",
        "        x_data       : (n_samples, n_features) predictor matrix\n",
        "        y_data       : (n_samples,) or (n_samples,1) response vector\n",
        "        n_components : number of PLS latent components to extract\n",
        "        max_iter     : max reweighting iterations per component\n",
        "        tol          : tolerance on change in sum(weights) for convergence\n",
        "        c            : Tukey bisquare tuning constant\n",
        "        verbose      : if True, print per‐component convergence info\n",
        "\n",
        "    Returns:\n",
        "        final_pls   : PLSRegression fitted on full weighted X,Y\n",
        "        final_wts   : final sample weights from last component (n_samples,)\n",
        "    \"\"\"\n",
        "    # Flatten y and median‐center X and Y for robustness\n",
        "    y = y_data.flatten()\n",
        "    x = x_data.copy()\n",
        "    x = x - np.median(x, axis=0)\n",
        "    y = y - np.median(y)\n",
        "\n",
        "    n, p = x.shape\n",
        "\n",
        "    # Initialize diagonal weight matrix D as identity scaled by 1/n\n",
        "    D = np.eye(n) * (1.0 / n)\n",
        "\n",
        "    # Prepare matrices to store W, T, P, Q across components\n",
        "    W = np.zeros((p, n_components))     # weight vectors\n",
        "    T = np.zeros((n, n_components))     # score vectors\n",
        "    P = np.zeros((p, n_components))     # loadings\n",
        "    Q = np.zeros(n_components)          # Y loadings\n",
        "\n",
        "    for comp in range(n_components):\n",
        "        if verbose:\n",
        "            print(f\"\\n--- Component {comp+1}/{n_components} ---\")\n",
        "        \n",
        "        # Initialize uniform weights for first iteration\n",
        "        d = np.full(n, 1.0/n)\n",
        "\n",
        "        for iteration in range(1, max_iter+1):\n",
        "            # Compute weighted covariance vector: v_t = X' D y\n",
        "            vt = x.T.dot(d * y)\n",
        "\n",
        "            # Normalize to get component weight vector\n",
        "            w = vt / np.linalg.norm(vt)\n",
        "\n",
        "            # Score vector (project data onto w)\n",
        "            t = x.dot(w)\n",
        "            t = t / np.linalg.norm(t)\n",
        "\n",
        "            # Regression coefficient on t\n",
        "            q = float((d * y).dot(t))\n",
        "\n",
        "            # Compute residual for robust weight update\n",
        "            r = y - t * q\n",
        "\n",
        "            # Estimate robust scale via MAD\n",
        "            med = np.median(r)\n",
        "            mad = np.median(np.abs(r - med))\n",
        "            scale = (mad * 1.4826) if mad != 0 else np.std(r)\n",
        "\n",
        "            # Apply Tukey bisquare weight function\n",
        "            u = r / scale\n",
        "            new_d = np.where(np.abs(u) < c, (1 - (u**2)/(c**2))**2, 0.0)\n",
        "\n",
        "            # Check for convergence of weight vector\n",
        "            if np.abs(new_d.sum() - d.sum()) < tol:\n",
        "                if verbose:\n",
        "                    print(f\" converged inner at iter {iteration}, Δsum={np.abs(new_d.sum()-d.sum()):.3e}\")\n",
        "                d = new_d\n",
        "                break\n",
        "\n",
        "            d = new_d\n",
        "        else:\n",
        "            if verbose:\n",
        "                print(f\" inner did not converge after {max_iter} its; last Δsum={np.abs(new_d.sum()-d.sum()):.3e}\")\n",
        "\n",
        "        # Store learned component vectors\n",
        "        W[:, comp] = w\n",
        "        T[:, comp] = t\n",
        "        Q[comp]    = q\n",
        "        P[:, comp] = x.T.dot(d * t)\n",
        "\n",
        "        # Deflate X and Y for next component\n",
        "        x = x - np.outer(t, P[:, comp])\n",
        "        y = y - t * q\n",
        "\n",
        "    # Compute final PLS model using sample weights from last component\n",
        "    final_weights = d\n",
        "    sqrt_w = np.sqrt(final_weights)\n",
        "    Xw = x_data * sqrt_w[:, None]     # weighted X\n",
        "    Yw = y_data.flatten() * sqrt_w    # weighted Y\n",
        "\n",
        "    # Fit final PLS regression model\n",
        "    final_pls = PLSRegression(n_components=n_components)\n",
        "    final_pls.fit(Xw, Yw)\n",
        "\n",
        "    return final_pls, final_weights\n",
        "\n",
        "def train_cross_validation_gridsearch(\n",
        "    x_train_data: np.ndarray,\n",
        "    y_train_data: np.ndarray,\n",
        "    c_values: List[float],\n",
        "    max_components: int,\n",
        "    n_splits: int,\n",
        "    random_state: int,\n",
        "    verbose: bool = False,\n",
        "    n_bins: int = 10\n",
        ") -> Tuple[pd.DataFrame, float, int, float]:\n",
        "    \"\"\"\n",
        "    Grid search over c values using stratified K-fold CV for regression.\n",
        "\n",
        "    Parameters:\n",
        "        x_train_data   : Training features\n",
        "        y_train_data   : Training targets\n",
        "        c_values       : List of c values to test\n",
        "        max_components : Max number of latent variables to test\n",
        "        n_splits       : Number of folds for StratifiedKFold\n",
        "        random_state   : Random seed\n",
        "        verbose        : Print progress\n",
        "        n_bins         : Number of bins to stratify y (default = 10)\n",
        "\n",
        "    Returns:\n",
        "        cv_results_df  : DataFrame with [c, knee_LV, knee_RMSECV]\n",
        "        cv_opt_rmsecv  : Best RMSECV found\n",
        "        cv_opt_A       : Optimal number of LVs\n",
        "        cv_opt_c       : Optimal c value\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    # Loop over all candidate c values\n",
        "    for c in c_values:\n",
        "        if verbose:\n",
        "            print(f\"\\nTesting c = {c:.2f}\")\n",
        "\n",
        "        rmsecv_list = []\n",
        "\n",
        "        # Loop over number of latent variables to test\n",
        "        for n_comp in range(1, max_components + 1):\n",
        "            mse_folds = []\n",
        "\n",
        "            # K-fold stratified CV\n",
        "            for train_idx, val_idx in stratified_cv_splits(x_train_data, y_train_data, n_splits, random_state, n_bins):\n",
        "                X_train_fold = x_train_data[train_idx]\n",
        "                X_val_fold   = x_train_data[val_idx]\n",
        "                y_train_fold = y_train_data[train_idx]\n",
        "                y_val_fold   = y_train_data[val_idx]\n",
        "\n",
        "                # Train irPLS model for given c and component count\n",
        "                pls_model, _ = model_irpls(\n",
        "                    x_data=X_train_fold,\n",
        "                    y_data=y_train_fold,\n",
        "                    n_components=n_comp,\n",
        "                    max_iter=100,\n",
        "                    tol=1e-6,\n",
        "                    c=c,\n",
        "                    verbose=False\n",
        "                )\n",
        "\n",
        "                # Predict and compute fold MSE\n",
        "                y_pred_val = pls_model.predict(X_val_fold).flatten()\n",
        "                mse_folds.append(mean_squared_error(y_val_fold, y_pred_val))\n",
        "\n",
        "            # Compute RMSECV for this component count\n",
        "            rmsecv = np.sqrt(np.mean(mse_folds))\n",
        "            rmsecv_list.append(rmsecv)\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"    LVs: {n_comp}, RMSECV: {rmsecv:.4f}\")\n",
        "\n",
        "        # Plot RMSECV curve for this c value\n",
        "        plt.figure(figsize=(8, 5))\n",
        "        plt.plot(range(1, max_components + 1), rmsecv_list, marker='o')\n",
        "        plt.xlabel(\"Number of Latent Variables\")\n",
        "        plt.ylabel(\"RMSECV\")\n",
        "        plt.title(f\"RMSECV Curve for c = {c:.2f}\")\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "        # Find the knee point (optimal number of components)\n",
        "        x_vals = list(range(1, max_components + 1))\n",
        "        knee_locator = KneeLocator(x_vals, rmsecv_list, curve=\"convex\", direction=\"decreasing\")\n",
        "        knee_LV = knee_locator.knee\n",
        "\n",
        "        # Fallback to min RMSECV if no knee found\n",
        "        if knee_LV is None:\n",
        "            if verbose:\n",
        "                print(\"No knee found, using min RMSECV as fallback.\")\n",
        "            knee_LV = np.argmin(rmsecv_list) + 1\n",
        "\n",
        "        # Store result for this c value\n",
        "        knee_rmsecv = rmsecv_list[knee_LV - 1]\n",
        "        results.append({\n",
        "            \"c\": c,\n",
        "            \"knee_LV\": knee_LV,\n",
        "            \"knee_RMSECV\": knee_rmsecv\n",
        "        })\n",
        "\n",
        "    # Compile results across all c values\n",
        "    cv_results_df = pd.DataFrame(results)\n",
        "\n",
        "    # Find best overall c value and corresponding model settings\n",
        "    best_idx = cv_results_df[\"knee_RMSECV\"].idxmin()\n",
        "    cv_opt_c = cv_results_df.loc[best_idx, \"c\"]\n",
        "    cv_opt_A = cv_results_df.loc[best_idx, \"knee_LV\"]\n",
        "    cv_opt_rmsecv = cv_results_df.loc[best_idx, \"knee_RMSECV\"]\n",
        "\n",
        "    if verbose:\n",
        "        print(\"\\n==== Best Overall ====\")\n",
        "        print(f\"Best c: {cv_opt_c:.2f}\")\n",
        "        print(f\"Best number of LVs: {cv_opt_A}\")\n",
        "        print(f\"Best RMSECV: {cv_opt_rmsecv:.4f}\")\n",
        "\n",
        "    return cv_results_df, cv_opt_rmsecv, cv_opt_A, cv_opt_c\n",
        "\n",
        "def test_irpls_model(\n",
        "    x_train_data: np.ndarray,\n",
        "    y_train_data: np.ndarray,\n",
        "    x_test_data: np.ndarray,\n",
        "    y_test_data: np.ndarray,\n",
        "    opt_A: int,\n",
        "    opt_c: float,\n",
        "    max_iter: int,\n",
        "    tol: float,\n",
        "    verbose: bool = False\n",
        ") -> Tuple[\n",
        "    PLSRegression,  # irpls_model\n",
        "    pd.DataFrame,   # test_irpls_results\n",
        "    float,          # test_rmsep\n",
        "    float,          # test_r2\n",
        "    float,          # test_practical_accuracy\n",
        "    float           # final_weights\n",
        "]:\n",
        "    \"\"\"\n",
        "    Train a final irPLS regression model on the training set using the specified number \n",
        "    of latent variables and bisquare tuning constant, then evaluate its performance on \n",
        "    the test set.\n",
        "    \"\"\"\n",
        "    # Fit irPLS model on the full training set\n",
        "    irpls_model, final_weights = model_irpls(\n",
        "        x_data=x_train_data,\n",
        "        y_data=y_train_data,\n",
        "        n_components=opt_A,\n",
        "        max_iter=max_iter,\n",
        "        tol=tol,\n",
        "        c=opt_c,\n",
        "        verbose=verbose\n",
        "    )\n",
        "\n",
        "    # Predict on test set\n",
        "    y_pred = irpls_model.predict(x_test_data).flatten()\n",
        "\n",
        "    # Flatten true labels\n",
        "    y_true = y_test_data.flatten()\n",
        "\n",
        "    # Compute RMSEP (Root Mean Squared Error of Prediction)\n",
        "    test_rmsep = float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "\n",
        "    # Compute R² (coefficient of determination)\n",
        "    test_r2 = float(r2_score(y_true, y_pred))\n",
        "\n",
        "    # Compute practical accuracy: percentage of predictions within ±20% of ground truth\n",
        "    pct_error = np.abs(y_pred - y_true) / np.abs(y_true)\n",
        "    test_practical_accuracy = float((pct_error <= 0.2).mean() * 100.0)\n",
        "\n",
        "    # Assemble result DataFrame with observed, predicted, and percentage error\n",
        "    test_irpls_results = pd.DataFrame({\n",
        "        \"observed\":  y_true,\n",
        "        \"predicted\": y_pred,\n",
        "        \"pct_error\": pct_error\n",
        "    })\n",
        "\n",
        "    # Print test performance summary\n",
        "    print(f\"Test RMSEP: {test_rmsep:.4f}\")\n",
        "    print(f\"Test R²: {test_r2:.4f}\")\n",
        "    print(f\"Practical accuracy (±20%): {test_practical_accuracy:.1f}%\")\n",
        "\n",
        "    # Plot parity plot (Observed vs Predicted)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(y_true, y_pred, alpha=0.7, label=\"Test Data\")\n",
        "    plt.plot(\n",
        "        [y_true.min(), y_true.max()],\n",
        "        [y_true.min(), y_true.max()],\n",
        "        \"k--\", lw=2, label=\"Ideal\"\n",
        "    )\n",
        "    plt.xlabel(\"Observed\")\n",
        "    plt.ylabel(\"Predicted\")\n",
        "    plt.title(\"Observed vs. Predicted on Test Set (irPLS)\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    return (\n",
        "        irpls_model,\n",
        "        test_irpls_results,\n",
        "        test_rmsep,\n",
        "        test_r2,\n",
        "        test_practical_accuracy,\n",
        "        final_weights\n",
        "    )"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1754473421551
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### _Variables_"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DF = df\n",
        "RANDOM_STATE = 27\n",
        "TEST_VARIETY = \"TestVariety\"\n",
        "TEST_SEASON = 2025\n",
        "MAX_COMPONENTS = 20\n",
        "N_SPLITS = 3\n",
        "MAX_ITER = 100\n",
        "C_RANGE = list(range(1, 31))\n",
        "TOL = 1e-6"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1754473426579
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### _Run_"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Split into train and test sets ===\n",
        "df_train, df_test_variety, df_test_season = split_train_test(\n",
        "    df,\n",
        "    test_variety=TEST_VARIETY,\n",
        "    test_season=TEST_SEASON\n",
        ")\n",
        "\n",
        "# === Convert to x and y arrays ===\n",
        "x_train, y_train = split_x_y(\n",
        "    df_train,\n",
        ")\n",
        "x_test_variety, y_train_variety = split_x_y(\n",
        "    df_test_variety,\n",
        ")\n",
        "x_test_season, y_train_season = split_x_y(\n",
        "    df_test_season,\n",
        ")\n",
        "\n",
        "# === Run gridsearch cross-validation ===\n",
        "cv_results_df, cv_opt_rmsecv, cv_opt_A, cv_opt_c = train_cross_validation_gridsearch(\n",
        "    x_train_data=x_train,\n",
        "    y_train_data=y_train,\n",
        "    c_values=C_RANGE,\n",
        "    max_components=MAX_COMPONENTS,\n",
        "    n_splits=N_SPLITS,\n",
        "    random_state=RANDOM_STATE,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(f\"Optimal # of components: {cv_opt_A} | Optimal c: {cv_opt_c:.2f} | CV RMSECV: {cv_opt_rmsecv:.4f}\")\n",
        "\n",
        "# === Test on VARIETY ===\n",
        "irpls_model_variety, results_variety, rmsep_variety, r2_variety, acc_variety, weights_variety = test_irpls_model(\n",
        "    x_train_data=x_train,\n",
        "    y_train_data=y_train,\n",
        "    x_test_data=x_test_variety,\n",
        "    y_test_data=y_train_variety,\n",
        "    opt_A=cv_opt_A,\n",
        "    opt_c=cv_opt_c,\n",
        "    max_iter=MAX_ITER,\n",
        "    tol=TOL,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# === Test on SEASON ===\n",
        "irpls_model_season, results_season, rmsep_season, r2_season, acc_season, weights_season = test_irpls_model(\n",
        "    x_train_data=x_train,\n",
        "    y_train_data=y_train,\n",
        "    x_test_data=x_test_season,\n",
        "    y_test_data=y_train_season,\n",
        "    opt_A=cv_opt_A,\n",
        "    opt_c=cv_opt_c,\n",
        "    max_iter=MAX_ITER,\n",
        "    tol=TOL,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# === Print summary ===\n",
        "print(\"\\n==== irPLS Test Results ====\")\n",
        "print(f\"Variety  | RMSEP: {rmsep_variety:.4f} | R²: {r2_variety:.4f} | Acc: {acc_variety:.2%}\")\n",
        "print(f\"Season   | RMSEP: {rmsep_season:.4f}  | R²: {r2_season:.4f}  | Acc: {acc_season:.2%}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1754473733557
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### _Inference Time Analysis_"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_inference_sample_set(\n",
        "    df_variety: pd.DataFrame,\n",
        "    df_season: pd.DataFrame,\n",
        "    random_state: int,\n",
        "    sample_size: int = 1000\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Combine two test sets (variety and season), sample rows randomly, and return X and y arrays.\n",
        "\n",
        "    Parameters:\n",
        "        df_variety   : DataFrame for variety-based test set\n",
        "        df_season    : DataFrame for season-based test set\n",
        "        random_state : Random seed for reproducibility\n",
        "        sample_size  : Number of rows to sample from combined test set\n",
        "\n",
        "    Returns:\n",
        "        x_sample : NumPy array of shape (sample_size, n_features) with spectral features\n",
        "        y_sample : NumPy array of shape (sample_size,) with corresponding Brix values\n",
        "    \"\"\"\n",
        "    # Combine the two test sets\n",
        "    df_combined = pd.concat([df_variety, df_season], axis=0)\n",
        "\n",
        "    # Randomly sample rows from the combined test set\n",
        "    df_sample = df_combined.sample(\n",
        "        n=sample_size,\n",
        "        random_state=random_state\n",
        "    )\n",
        "\n",
        "    # Split into X and y arrays\n",
        "    x_sample, y_sample = split_x_y(df_sample)\n",
        "\n",
        "    return x_sample, y_sample\n",
        "\n",
        "def test_irpls_inference_time(\n",
        "    x_train: np.ndarray,\n",
        "    y_train: np.ndarray,\n",
        "    x_test: np.ndarray,\n",
        "    y_test: np.ndarray,\n",
        "    n_components: int,\n",
        "    c: float,\n",
        "    max_iter: int,\n",
        "    tol: float\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Train irPLS with specified hyperparameters and return average inference time per sample (in ms).\n",
        "\n",
        "    Parameters:\n",
        "        x_train      : Training features\n",
        "        y_train      : Training targets\n",
        "        x_test       : Test features\n",
        "        y_test       : Test targets\n",
        "        n_components : Number of irPLS latent variables\n",
        "        c            : Tukey bisquare tuning constant\n",
        "        max_iter     : Maximum number of iterations per component\n",
        "        tol          : Convergence threshold for weight updates\n",
        "\n",
        "    Returns:\n",
        "        avg_inference_time_ms : Average inference time per sample in milliseconds\n",
        "    \"\"\"\n",
        "    # Fit irPLS model on the training data\n",
        "    irpls_model, _ = model_irpls(\n",
        "        x_data=x_train,\n",
        "        y_data=y_train,\n",
        "        n_components=n_components,\n",
        "        max_iter=max_iter,\n",
        "        tol=tol,\n",
        "        c=c,\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    times = []\n",
        "\n",
        "    # Predict each test sample individually and measure the time\n",
        "    for x in x_test:\n",
        "        x_input = x.reshape(1, -1)\n",
        "        start = time.time()\n",
        "        _ = irpls_model.predict(x_input)\n",
        "        end = time.time()\n",
        "        times.append(end - start)\n",
        "\n",
        "    # Compute average inference time in milliseconds\n",
        "    avg_inference_time_ms = np.mean(times) * 1000\n",
        "\n",
        "    # Print result\n",
        "    print(f\"Average inference time (irPLS): {avg_inference_time_ms:.3f} ms/sample\")\n",
        "\n",
        "    return avg_inference_time_ms\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1754474272900
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Create sample set for inference time measurement ===\n",
        "x_inference_time, y_inference_time = get_inference_sample_set(\n",
        "    df_test_variety,\n",
        "    df_test_season,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "# === Compute the average inference time ===\n",
        "inference_time_irpls = test_irpls_inference_time(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    x_inference_time,\n",
        "    y_inference_time,\n",
        "    n_components=cv_opt_A,\n",
        "    c=cv_opt_c,\n",
        "    max_iter=MAX_ITER,\n",
        "    tol=TOL\n",
        ")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1754474322907
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.16",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}