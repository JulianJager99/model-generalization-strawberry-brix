{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### _Setup_"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset memory\n",
        "%reset -f"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1754487751803
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install correct package versions\n",
        "!pip install \"tensorflow[and-cuda]\"\n",
        "!pip uninstall numpy pandas -y\n",
        "!pip install \"numpy<2.0\" pandas --upgrade --no-cache-dir"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1754487777718
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Packages\n",
        "from typing import Union, List, Tuple, Dict, Any\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold, StratifiedKFold, StratifiedShuffleSplit, train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers, initializers, optimizers, callbacks\n",
        "from tensorflow.keras.layers import Layer\n",
        "import tensorflow_probability as tfp\n",
        "import optuna\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tfpl = tfp.layers\n",
        "tfd = tfp.distributions"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1754487799783
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU check\n",
        "print(\"Available GPUs:\", tf.config.list_physical_devices('GPU'))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1754487801848
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data\n",
        "df = pd.read_csv('data.csv')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1754487826671
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### _Functions_"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_col_index_of_spectra(\n",
        "    df: pd.DataFrame\n",
        ") -> int:\n",
        "    \"\"\"\n",
        "    Find the column index where spectral data starts.\n",
        "\n",
        "    Assumes spectral column names can be converted to float (e.g., \"730.5\", \"731.0\").\n",
        "\n",
        "    Parameters:\n",
        "        df : Input DataFrame\n",
        "\n",
        "    Returns:\n",
        "        Index of the first spectral column, or -1 if not found.\n",
        "    \"\"\"\n",
        "    for idx, col in enumerate(df.columns):\n",
        "        try:\n",
        "            float(col)\n",
        "            return idx\n",
        "        except (ValueError, TypeError):\n",
        "            continue\n",
        "    return -1\n",
        "\n",
        "def split_train_test(\n",
        "    df: pd.DataFrame,\n",
        "    test_variety: str,\n",
        "    test_season: int       \n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Split a DataFrame into one training set and two test sets:\n",
        "\n",
        "    - Variety test set: Variety == test_variety AND Year == 2024\n",
        "    - Season test set : Year == test_season \n",
        "\n",
        "    The training set excludes all rows that belong to any of the test sets.\n",
        "    The season test set only includes varieties that are present in the training set.\n",
        "\n",
        "    Parameters:\n",
        "        df           : Full pandas DataFrame\n",
        "        test_variety : Variety used for the test set\n",
        "        test_season  : Year used for the season test\n",
        "\n",
        "    Returns:\n",
        "        df_train        : Training set\n",
        "        df_test_variety : Test set for specified variety and 2024\n",
        "        df_test_season  : Test set for specified season (filtered by train varieties)\n",
        "    \"\"\"\n",
        "\n",
        "    # Select test set for the specified variety in year 2024\n",
        "    df_test_variety = df[\n",
        "        (df[\"Variety\"] == test_variety) &\n",
        "        (df[\"Scan Date Year\"] == 2024)\n",
        "    ]\n",
        "\n",
        "    # Select test set for the specified season (regardless of variety)\n",
        "    df_test_season = df[\n",
        "        df[\"Scan Date Year\"] == test_season\n",
        "    ]\n",
        "\n",
        "    # Select training set (exclude test variety and test season)\n",
        "    df_train = df[\n",
        "        (df[\"Variety\"] != test_variety) &\n",
        "        (df[\"Scan Date Year\"] != test_season)\n",
        "    ]\n",
        "\n",
        "    # Filter season test set to only include varieties present in training set\n",
        "    train_varieties = df_train[\"Variety\"].unique()\n",
        "    df_test_season = df_test_season[\n",
        "        df_test_season[\"Variety\"].isin(train_varieties)\n",
        "    ]\n",
        "\n",
        "    return df_train, df_test_variety, df_test_season\n",
        "\n",
        "def split_x_y(\n",
        "    df: pd.DataFrame,\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Split a DataFrame into X (spectral features) and y (target) arrays.\n",
        "    Assumes find_col_index_of_spectra() is defined globally and returns the index\n",
        "    where spectral data starts.\n",
        "\n",
        "    Parameters:\n",
        "        df : Input DataFrame containing both metadata and spectral data.\n",
        "\n",
        "    Returns:\n",
        "        x : NumPy array of shape (n_samples, n_spectral_features)\n",
        "        y : NumPy array of shape (n_samples, 1) containing Brix values\n",
        "    \"\"\"\n",
        "    # Identify spectral columns (those that can be cast to float, e.g. wavelengths)\n",
        "    spectra_cols = list(df.columns[find_col_index_of_spectra(df):])\n",
        "\n",
        "    # Define the target column\n",
        "    target_cols = ['Brix (Position)']\n",
        "\n",
        "    # Extract feature and target arrays\n",
        "    x = df[spectra_cols].values\n",
        "    y = df[target_cols].values\n",
        "\n",
        "    return x, y\n",
        "\n",
        "def take_subset(\n",
        "    df: pd.DataFrame, \n",
        "    n_subset: int,\n",
        "    random_state: int\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Return a stratified subset of the DataFrame based on 10 Brix bins.\n",
        "\n",
        "    If n_subset >= len(df), the original DataFrame is returned.\n",
        "\n",
        "    Parameters:\n",
        "        df       : Input DataFrame with 'Brix (Position)' column\n",
        "        n_subset : Desired subset size\n",
        "        random_state : Random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "        Subset of df with stratification over 10 quantile bins of Brix\n",
        "    \"\"\"\n",
        "    # If requested subset size exceeds full dataset, return a copy of the full DataFrame\n",
        "    if n_subset >= len(df):\n",
        "        return df.copy()\n",
        "\n",
        "    # Bin the Brix values into 10 quantile-based bins for stratification\n",
        "    binned = pd.qcut(df[\"Brix (Position)\"], q=10, labels=False, duplicates='drop')\n",
        "\n",
        "    # Initialize stratified sampler\n",
        "    splitter = StratifiedShuffleSplit(\n",
        "        n_splits=1,\n",
        "        train_size=n_subset,\n",
        "        random_state=random_state\n",
        "    )\n",
        "\n",
        "    # Perform stratified split and extract subset indices\n",
        "    idx_subset, _ = next(splitter.split(df, binned))\n",
        "\n",
        "    # Return the stratified subset as a new DataFrame with reset index\n",
        "    return df.iloc[idx_subset].reset_index(drop=True)\n",
        "\n",
        "def create_train_val_split(\n",
        "    df: pd.DataFrame,\n",
        "    validation_size: float,\n",
        "    random_state: int\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Split a DataFrame into train and validation sets using stratified sampling\n",
        "    based on 10 quantile bins of the 'Brix (Position)' column.\n",
        "\n",
        "    Parameters:\n",
        "        df              : Input DataFrame\n",
        "        validation_size : Proportion of validation samples (0 < float < 1)\n",
        "        random_state    : Seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "        df_train, df_val : Stratified training and validation DataFrames\n",
        "    \"\"\"\n",
        "    # Bin the Brix values into 10 quantile-based bins for stratified splitting\n",
        "    binned = pd.qcut(df[\"Brix (Position)\"], q=10, labels=False, duplicates=\"drop\")\n",
        "\n",
        "    # Perform stratified train/validation split based on the binned Brix values\n",
        "    df_train, df_val = train_test_split(\n",
        "        df,\n",
        "        test_size=validation_size,\n",
        "        random_state=random_state,\n",
        "        stratify=binned\n",
        "    )\n",
        "\n",
        "    # Return splits with reset indices\n",
        "    return df_train.reset_index(drop=True), df_val.reset_index(drop=True)\n",
        "\n",
        "def rmse_loss(\n",
        "    y_true, \n",
        "    y_pred\n",
        "):\n",
        "    \"\"\"\n",
        "    Compute the Root Mean Squared Error (RMSE) as a loss function.\n",
        "\n",
        "    Parameters:\n",
        "        y_true : Tensor of true target values\n",
        "        y_pred : Tensor of predicted values\n",
        "\n",
        "    Returns:\n",
        "        RMSE as a scalar Tensor\n",
        "    \"\"\"\n",
        "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))  \n",
        "\n",
        "def rmse_metric(\n",
        "    y_true, \n",
        "    y_pred\n",
        "):\n",
        "    \"\"\"\n",
        "    Compute the Root Mean Squared Error (RMSE) as a performance metric.\n",
        "\n",
        "    Parameters:\n",
        "        y_true : Tensor of true target values\n",
        "        y_pred : Tensor of predicted values\n",
        "\n",
        "    Returns:\n",
        "        RMSE as a scalar Tensor\n",
        "    \"\"\"\n",
        "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))  \n",
        "\n",
        "def bcnn_model(\n",
        "    input_shape: int,\n",
        "    kernel_size: int,\n",
        "    dropout_rate: float,\n",
        "    l2_strength: float,\n",
        "    learning_rate: float,\n",
        "    random_state: int,\n",
        "    kl_scale: float\n",
        ") -> tf.keras.Model:\n",
        "    \"\"\"\n",
        "    Build and compile a Bayesian Convolutional Neural Network (BCNN) model.\n",
        "\n",
        "    The model includes:\n",
        "    - A deterministic 1D convolutional layer with L2 regularization.\n",
        "    - Bayesian dense layers using variational inference (DenseReparameterization).\n",
        "    - Dropout for regularization.\n",
        "    - KL divergence scaled by `kl_scale` for regularization of Bayesian layers.\n",
        "\n",
        "    Parameters:\n",
        "        input_shape    : Number of input features.\n",
        "        kernel_size    : Convolutional kernel size.\n",
        "        dropout_rate   : Dropout rate applied after key layers.\n",
        "        l2_strength    : L2 regularization strength for the Conv1D layer.\n",
        "        learning_rate  : Learning rate for the Adam optimizer.\n",
        "        random_state   : Random seed for weight initialization.\n",
        "        kl_scale       : Scaling factor for KL divergence in Bayesian layers.\n",
        "\n",
        "    Returns:\n",
        "        model : Compiled Keras model ready for training.\n",
        "    \"\"\"\n",
        "    # Define L2 regularizer and HeNormal initializer for Conv1D\n",
        "    kernel_reg  = regularizers.l2(l2_strength)\n",
        "    kernel_init = initializers.HeNormal(seed=random_state)\n",
        "\n",
        "    model = models.Sequential([\n",
        "        # Input layer reshaping the flat input into 1D format for Conv1D\n",
        "        tf.keras.Input(shape=(input_shape,)),\n",
        "        layers.Reshape((input_shape, 1)),\n",
        "\n",
        "        # Deterministic 1D convolutional layer\n",
        "        layers.Conv1D(\n",
        "            filters=1,\n",
        "            kernel_size=kernel_size,\n",
        "            padding=\"same\",\n",
        "            activation=\"elu\",\n",
        "            kernel_initializer=kernel_init,\n",
        "            kernel_regularizer=kernel_reg\n",
        "        ),\n",
        "\n",
        "        # Dropout regularization\n",
        "        layers.Dropout(dropout_rate),\n",
        "        layers.Flatten(),\n",
        "\n",
        "        # First Bayesian dense layer (variational)\n",
        "        tfpl.DenseReparameterization(\n",
        "            units=36,\n",
        "            activation=\"elu\",\n",
        "            kernel_prior_fn=tfpl.default_multivariate_normal_fn,\n",
        "            kernel_posterior_fn=tfpl.default_mean_field_normal_fn(),\n",
        "            kernel_divergence_fn=lambda q, p, _: tfd.kl_divergence(q, p) * kl_scale\n",
        "        ),\n",
        "        layers.Dropout(dropout_rate),\n",
        "\n",
        "        # Second Bayesian dense layer\n",
        "        tfpl.DenseReparameterization(\n",
        "            units=18,\n",
        "            activation=\"elu\",\n",
        "            kernel_prior_fn=tfpl.default_multivariate_normal_fn,\n",
        "            kernel_posterior_fn=tfpl.default_mean_field_normal_fn(),\n",
        "            kernel_divergence_fn=lambda q, p, _: tfd.kl_divergence(q, p) * kl_scale\n",
        "        ),\n",
        "        layers.Dropout(dropout_rate),\n",
        "\n",
        "        # Third Bayesian dense layer\n",
        "        tfpl.DenseReparameterization(\n",
        "            units=12,\n",
        "            activation=\"elu\",\n",
        "            kernel_prior_fn=tfpl.default_multivariate_normal_fn,\n",
        "            kernel_posterior_fn=tfpl.default_mean_field_normal_fn(),\n",
        "            kernel_divergence_fn=lambda q, p, _: tfd.kl_divergence(q, p) * kl_scale\n",
        "        ),\n",
        "\n",
        "        # Output Bayesian dense layer (linear activation for regression)\n",
        "        tfpl.DenseReparameterization(\n",
        "            units=1,\n",
        "            activation=\"linear\",\n",
        "            kernel_prior_fn=tfpl.default_multivariate_normal_fn,\n",
        "            kernel_posterior_fn=tfpl.default_mean_field_normal_fn(),\n",
        "            kernel_divergence_fn=lambda q, p, _: tfd.kl_divergence(q, p) * kl_scale\n",
        "        )\n",
        "    ])\n",
        "\n",
        "    # Compile model with RMSE loss and metric\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "        loss=lambda y_true, y_pred: tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true))),\n",
        "        metrics=[lambda y_true, y_pred: tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "def train_bcnn(\n",
        "    x_train: np.ndarray,\n",
        "    y_train: np.ndarray,\n",
        "    input_shape: int,\n",
        "    kernel_size: int,\n",
        "    dropout_rate: float,\n",
        "    l2_strength: float,\n",
        "    random_state: int,\n",
        "    kl_scale: float,\n",
        "    batch_size: int,\n",
        "    epochs: int,\n",
        "    patience_reduce_lr: int,\n",
        "    patience_early_stop: int,\n",
        "    min_lr: float,\n",
        "    x_val: np.ndarray,\n",
        "    y_val: np.ndarray,\n",
        "    verbose: int = 0\n",
        ") -> Tuple[tf.keras.Model, tf.keras.callbacks.History]:\n",
        "    \"\"\"\n",
        "    Train a Bayesian Convolutional Neural Network (BCNN) model with optional validation.\n",
        "\n",
        "    Uses KL-divergence regularized variational layers, with learning rate scheduling\n",
        "    and early stopping support.\n",
        "\n",
        "    Parameters:\n",
        "        x_train, y_train       : Training data and labels\n",
        "        input_shape            : Number of spectral input features\n",
        "        kernel_size            : Convolutional kernel size\n",
        "        dropout_rate           : Dropout rate for regularization\n",
        "        l2_strength            : L2 penalty for deterministic Conv1D layer\n",
        "        random_state           : Seed for reproducibility\n",
        "        kl_scale               : Weighting for KL divergence in variational layers\n",
        "        batch_size             : Mini-batch size\n",
        "        epochs                 : Maximum number of training epochs\n",
        "        patience_reduce_lr     : Patience for learning rate reduction callback\n",
        "        patience_early_stop    : Patience for early stopping callback\n",
        "        min_lr                 : Minimum learning rate allowed by scheduler\n",
        "        x_val, y_val           : Optional validation set\n",
        "        verbose                : Verbosity level (0 = silent, 1 = progress)\n",
        "\n",
        "    Returns:\n",
        "        model   : Trained BCNN model\n",
        "        history : Keras training history object\n",
        "    \"\"\"\n",
        "\n",
        "    # Build the Bayesian CNN model\n",
        "    model = bcnn_model(\n",
        "        input_shape=input_shape,\n",
        "        kernel_size=kernel_size,\n",
        "        dropout_rate=dropout_rate,\n",
        "        l2_strength=l2_strength,\n",
        "        learning_rate=0.01 * batch_size / 256,\n",
        "        random_state=random_state,\n",
        "        kl_scale=kl_scale\n",
        "    )\n",
        "\n",
        "    # Determine whether to monitor training loss or validation loss\n",
        "    if x_val is not None and y_val is not None:\n",
        "        monitor_metric = \"val_loss\"\n",
        "        validation_data = (x_val, y_val)\n",
        "    else:\n",
        "        monitor_metric = \"loss\"\n",
        "        validation_data = None\n",
        "\n",
        "    # Configure training callbacks for learning rate scheduling and early stopping\n",
        "    cb = [\n",
        "        callbacks.ReduceLROnPlateau(\n",
        "            monitor=monitor_metric,\n",
        "            factor=0.5,\n",
        "            patience=patience_reduce_lr,\n",
        "            min_lr=min_lr,\n",
        "            verbose=0\n",
        "        ),\n",
        "        callbacks.EarlyStopping(\n",
        "            monitor=monitor_metric,\n",
        "            patience=patience_early_stop,\n",
        "            restore_best_weights=True,\n",
        "            verbose=0\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Train the model with optional validation\n",
        "    history = model.fit(\n",
        "        x_train,\n",
        "        y_train,\n",
        "        validation_data=validation_data,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        callbacks=cb,\n",
        "        verbose=verbose\n",
        "    )\n",
        "\n",
        "    return model, history\n",
        "\n",
        "def perform_optuna_hyperparameter_optimization(\n",
        "    x_train: np.ndarray,\n",
        "    y_train: np.ndarray,\n",
        "    x_val: np.ndarray,\n",
        "    y_val: np.ndarray,\n",
        "    input_shape: int,\n",
        "    random_state: int,\n",
        "    epochs: int,\n",
        "    patience_reduce_lr: int,\n",
        "    patience_early_stop: int,\n",
        "    min_lr: float,\n",
        "    kernel_size_range: Tuple[int, int],\n",
        "    batch_size_list: list,\n",
        "    dropout_range: Tuple[float, float],\n",
        "    l2_range: Tuple[float, float],\n",
        "    kl_range: Tuple[float, float],\n",
        "    timeout_time: float\n",
        ") -> Tuple['optuna.study.Study', float, dict]:\n",
        "    \"\"\"\n",
        "    Perform Optuna-based hyperparameter tuning for a Bayesian CNN (BCNN) model.\n",
        "\n",
        "    Parameters:\n",
        "        x_train, y_train       : Training features and labels\n",
        "        x_val, y_val           : Validation features and labels\n",
        "        input_shape            : Number of spectral input features\n",
        "        random_state           : Seed for reproducibility\n",
        "        epochs                 : Maximum number of training epochs\n",
        "        patience_reduce_lr     : ReduceLROnPlateau callback patience\n",
        "        patience_early_stop    : EarlyStopping callback patience\n",
        "        min_lr                 : Minimum learning rate for LR scheduler\n",
        "        kernel_size_range      : Tuple (min, max) kernel sizes to test\n",
        "        batch_size_list        : List of candidate batch sizes\n",
        "        dropout_range          : Tuple (min, max) dropout values\n",
        "        l2_range               : Tuple (min, max) L2 regularization strengths\n",
        "        kl_range               : Tuple (min, max) KL divergence weights\n",
        "        timeout_time           : Max tuning time allowed (in seconds)\n",
        "\n",
        "    Returns:\n",
        "        study          : Optuna study object\n",
        "        best_val_rmse  : Best RMSE on validation set\n",
        "        best_params    : Dictionary of best trial hyperparameters\n",
        "    \"\"\"\n",
        "\n",
        "    def objective(trial):\n",
        "        # Suggest hyperparameters using Optuna's search space\n",
        "        kernel_size   = trial.suggest_int(\"kernel_size\", kernel_size_range[0], kernel_size_range[1])\n",
        "        batch_size    = trial.suggest_categorical(\"batch_size\", batch_size_list)\n",
        "        dropout_rate  = trial.suggest_float(\"dropout_rate\", dropout_range[0], dropout_range[1])\n",
        "        l2_strength   = trial.suggest_float(\"l2_strength\", l2_range[0], l2_range[1], log=True)\n",
        "        kl_scale      = trial.suggest_float(\"kl_scale\", kl_range[0], kl_range[1], log=True)\n",
        "\n",
        "        # Learning rate is scaled based on batch size\n",
        "        learning_rate = 0.01 * batch_size / 256\n",
        "\n",
        "        # Print trial configuration\n",
        "        print(f\"\\n[Optuna Trial {trial.number}] Hyperparameters:\")\n",
        "        print(f\"  kernel_size   = {kernel_size}\")\n",
        "        print(f\"  batch_size    = {batch_size}\")\n",
        "        print(f\"  dropout_rate  = {dropout_rate:.4f}\")\n",
        "        print(f\"  l2_strength   = {l2_strength:.2e}\")\n",
        "        print(f\"  learning_rate = {learning_rate:.2e}\")\n",
        "        print(f\"  kl_scale      = {kl_scale:.2e}\")\n",
        "\n",
        "        # Train the model using the suggested parameters\n",
        "        model, history = train_bcnn(\n",
        "            x_train=x_train,\n",
        "            y_train=y_train,\n",
        "            x_val=x_val,\n",
        "            y_val=y_val,\n",
        "            input_shape=input_shape,\n",
        "            kernel_size=kernel_size,\n",
        "            dropout_rate=dropout_rate,\n",
        "            l2_strength=l2_strength,\n",
        "            random_state=random_state,\n",
        "            kl_scale=kl_scale,\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            patience_reduce_lr=patience_reduce_lr,\n",
        "            patience_early_stop=patience_early_stop,\n",
        "            min_lr=min_lr,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        # Evaluate model on validation set\n",
        "        y_pred = model.predict(x_val, batch_size=batch_size, verbose=0).flatten()\n",
        "        y_true = y_val.flatten()\n",
        "        val_rmse = float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "        print(f\"  Validation RMSE: {val_rmse:.5f}\")\n",
        "\n",
        "        return val_rmse\n",
        "\n",
        "    # Create and run Optuna study\n",
        "    study = optuna.create_study(direction=\"minimize\")\n",
        "    study.optimize(objective, timeout=timeout_time)\n",
        "\n",
        "    # Extract best result\n",
        "    best_val_rmse = study.best_value\n",
        "    best_params = study.best_trial.params\n",
        "\n",
        "    return study, best_val_rmse, best_params\n",
        "\n",
        "def test_bcnn(\n",
        "    model: tf.keras.Model,\n",
        "    x_test_data: np.ndarray,\n",
        "    y_test_data: np.ndarray,\n",
        "    batch_size: int,\n",
        "    num_monte_carlo: int\n",
        ") -> tuple:\n",
        "    \"\"\"\n",
        "    Evaluate a trained Bayesian CNN model on a hold-out test set using Monte Carlo sampling.\n",
        "\n",
        "    Parameters:\n",
        "        model            : Trained BCNN Keras model\n",
        "        x_test_data      : Test input features\n",
        "        y_test_data      : True target values\n",
        "        batch_size       : Batch size for model prediction\n",
        "        num_monte_carlo  : Number of MC passes to sample model uncertainty\n",
        "\n",
        "    Returns:\n",
        "        test_rmsep              : Root mean square error of prediction\n",
        "        test_r2                 : R² score\n",
        "        test_practical_accuracy: % predictions within ±20% of true value\n",
        "        df_predictions          : DataFrame with MC samples, predicted mean, and ground truth\n",
        "    \"\"\"\n",
        "    # Perform MC passes to estimate predictive uncertainty\n",
        "    mc_preds = []\n",
        "    for i in range(num_monte_carlo):\n",
        "        preds = model.predict(x_test_data, batch_size=batch_size, verbose=0)\n",
        "        mc_preds.append(preds.flatten())\n",
        "\n",
        "    # Stack MC predictions into [n_samples, num_monte_carlo] shape\n",
        "    mc_preds = np.stack(mc_preds, axis=1)\n",
        "\n",
        "    # Create DataFrame with all MC samples\n",
        "    df_predictions = pd.DataFrame(\n",
        "        mc_preds, \n",
        "        columns=[f\"mc_pass_{i+1}\" for i in range(num_monte_carlo)]\n",
        "    )\n",
        "\n",
        "    # Compute mean prediction and add to DataFrame with observed values\n",
        "    df_predictions[\"predicted\"] = df_predictions.mean(axis=1)\n",
        "    df_predictions[\"observed\"] = y_test_data.flatten()\n",
        "\n",
        "    # Evaluation metrics\n",
        "    y_pred = df_predictions[\"predicted\"].values\n",
        "    y_true = df_predictions[\"observed\"].values\n",
        "\n",
        "    test_rmsep = float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "    test_r2 = float(r2_score(y_true, y_pred))\n",
        "    pct_error = np.abs(y_pred - y_true) / np.abs(y_true)\n",
        "    test_practical_accuracy = float((pct_error <= 0.2).mean() * 100.0)\n",
        "\n",
        "    # Print evaluation results\n",
        "    print(f\"Test RMSEP: {test_rmsep:.4f}\")\n",
        "    print(f\"Test R²: {test_r2:.4f}\")\n",
        "    print(f\"Practical accuracy (±20%): {test_practical_accuracy:.1f}%\")\n",
        "\n",
        "    # Parity plot: predicted vs. observed\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(y_true, y_pred, alpha=0.7, label=\"Test Data\")\n",
        "    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], \"k--\", lw=2, label=\"Ideal\")\n",
        "    plt.xlabel(\"Observed\")\n",
        "    plt.ylabel(\"Predicted\")\n",
        "    plt.title(\"Observed vs. Predicted on Test Set (BCNN)\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    return test_rmsep, test_r2, test_practical_accuracy, df_predictions\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1754487837156
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### _Parameters_"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DF                              = df\n",
        "N_SUBSET                        = 23690\n",
        "VALIDATION_SIZE                 = 0.1\n",
        "TEST_VARIETY                    = \"TestVariety\"\n",
        "TEST_SEASON                     = 2025\n",
        "RANDOM_STATE                    = 27\n",
        "\n",
        "PATIENCE_CALLBACK_REDUCE_LR     = 25\n",
        "PATIENCE_CALLBACK_EARLY_STOP    = 50\n",
        "MIN_LR                          = 1e-6\n",
        "\n",
        "KERNEL_SIZE_RANGE               = (3, 1025)\n",
        "BATCH_SIZE_OPTIONS              = [32, 64, 128, 256, 512, 1024]     \n",
        "DROPOUT_RANGE                   = (0.01, 0.4)\n",
        "L2_RANGE                        = (1e-6, 1e-2)\n",
        "KL_SCALE_RANGE                  = (1e-6, 1e-2)\n",
        "TIMEOUT_TIME                    = 60 * 60 * 72\n",
        "\n",
        "TRAIN_EPOCHS                    = 250\n",
        "TEST_EPOCHS                     = 1000\n",
        "NUM_MONTE_CARLO                 = 100"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1754487838440
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"tensorflow_probability\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1754487843445
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### _Run_"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Split into train and test sets ===\n",
        "df_train_all, df_test_variety, df_test_season = split_train_test(\n",
        "    df,\n",
        "    test_variety=TEST_VARIETY,\n",
        "    test_season=TEST_SEASON\n",
        ")\n",
        "\n",
        "# === Take subset ===\n",
        "df_subset = take_subset(\n",
        "    df_train_all, \n",
        "    n_subset=N_SUBSET, \n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "# === Make train/validation split ===\n",
        "df_train, df_val = create_train_val_split(\n",
        "    df=df_subset,\n",
        "    validation_size=VALIDATION_SIZE,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "# === Convert to x and y arrays ===\n",
        "x_train_all, y_train_all = split_x_y(\n",
        "    df_train_all,\n",
        ")\n",
        "x_train, y_train = split_x_y(\n",
        "    df_train,\n",
        ")\n",
        "x_val, y_val = split_x_y(\n",
        "    df_val,\n",
        ")\n",
        "x_test_variety, y_test_variety = split_x_y(\n",
        "    df_test_variety,\n",
        ")\n",
        "x_test_season, y_test_season = split_x_y(\n",
        "    df_test_season,\n",
        ")\n",
        "\n",
        "study, best_val_rmse, best_params = perform_optuna_hyperparameter_optimization(\n",
        "    x_train=x_train,\n",
        "    y_train=y_train,\n",
        "    x_val=x_val,\n",
        "    y_val=y_val,\n",
        "    input_shape=x_train.shape[1],\n",
        "    random_state=RANDOM_STATE,\n",
        "    epochs=TRAIN_EPOCHS,\n",
        "    patience_reduce_lr=PATIENCE_CALLBACK_REDUCE_LR,\n",
        "    patience_early_stop=PATIENCE_CALLBACK_EARLY_STOP,\n",
        "    min_lr=MIN_LR,\n",
        "    kernel_size_range=KERNEL_SIZE_RANGE,\n",
        "    batch_size_list=BATCH_SIZE_OPTIONS,\n",
        "    dropout_range=DROPOUT_RANGE,\n",
        "    l2_range=L2_RANGE,\n",
        "    kl_range=KL_SCALE_RANGE,\n",
        "    timeout_time=TIMEOUT_TIME\n",
        ")\n",
        "\n",
        "print(\"\\nBest params:\", best_params)\n",
        "print(\"Best validation RMSE:\", best_val_rmse)\n",
        "\n",
        "# === Retrain the BCNN on all training data with the optimal hyperparameter setting ===\n",
        "bcnn_trained, _ = train_bcnn(\n",
        "    x_train=x_train_all,\n",
        "    y_train=y_train_all,\n",
        "    x_val=None,\n",
        "    y_val=None,\n",
        "    input_shape=x_train_all.shape[1],\n",
        "    kernel_size=best_params[\"kernel_size\"],\n",
        "    dropout_rate=best_params[\"dropout_rate\"],\n",
        "    l2_strength=best_params[\"l2_strength\"],\n",
        "    random_state=RANDOM_STATE,\n",
        "    kl_scale=best_params[\"kl_scale\"],\n",
        "    batch_size=best_params[\"batch_size\"],\n",
        "    epochs=TEST_EPOCHS,\n",
        "    patience_reduce_lr=PATIENCE_CALLBACK_REDUCE_LR,\n",
        "    patience_early_stop=PATIENCE_CALLBACK_EARLY_STOP,\n",
        "    min_lr=MIN_LR\n",
        ")\n",
        "\n",
        "# === Test on the test sets ===\n",
        "rmsep_variety, r2_variety, acc_variety, df_predictions_variety = test_bcnn(\n",
        "    model=bcnn_trained,\n",
        "    x_test_data=x_test_variety,\n",
        "    y_test_data=y_test_variety,\n",
        "    batch_size=best_params[\"batch_size\"],\n",
        "    num_monte_carlo=NUM_MONTE_CARLO\n",
        ")\n",
        "print(f\"VARIETY: RMSEP={rmsep_variety:.3f}, R2={r2_variety:.3f}, ACC(±20%)={acc_variety:.1f}%s\")\n",
        "\n",
        "rmsep_season, r2_season, acc_season, df_predictions_season = test_bcnn(\n",
        "    model=bcnn_trained,\n",
        "    x_test_data=x_test_season,\n",
        "    y_test_data=y_test_season,\n",
        "    batch_size=best_params[\"batch_size\"],\n",
        "    num_monte_carlo=NUM_MONTE_CARLO\n",
        ")\n",
        "print(f\"SEASON:  RMSEP={rmsep_season:.3f}, R2={r2_season:.3f}, ACC(±20%)={acc_season:.1f}%s\")\n",
        "\n",
        "# === Gather results ===\n",
        "results = {\n",
        "    \"Test Set\": [\"VARIETY\", \"SEASON\"],\n",
        "    \"RMSEP\": [rmsep_variety, rmsep_season],\n",
        "    \"R2\": [r2_variety, r2_season],\n",
        "    \"Accuracy (±20%)\": [acc_variety, acc_season]\n",
        "}\n",
        "\n",
        "# Create a DataFrame\n",
        "df_results = pd.DataFrame(results)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1754487915096
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### _Inference Time Analysis_"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_inference_sample_set(\n",
        "    df_variety: pd.DataFrame,\n",
        "    df_season: pd.DataFrame,\n",
        "    random_state: int,\n",
        "    sample_size: int = 1000\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Combine two test sets (variety and season), sample rows randomly, and return X and y arrays.\n",
        "\n",
        "    Parameters:\n",
        "        df_variety   : DataFrame for variety-based test set\n",
        "        df_season    : DataFrame for season-based test set\n",
        "        random_state : Random seed for reproducibility\n",
        "        sample_size  : Number of rows to sample from combined test set\n",
        "\n",
        "    Returns:\n",
        "        x_sample : NumPy array of shape (sample_size, n_features) with spectral features\n",
        "        y_sample : NumPy array of shape (sample_size,) with corresponding Brix values\n",
        "    \"\"\"\n",
        "    # Combine the two test sets\n",
        "    df_combined = pd.concat([df_variety, df_season], axis=0)\n",
        "\n",
        "    # Randomly sample rows from the combined test set\n",
        "    df_sample = df_combined.sample(\n",
        "        n=sample_size,\n",
        "        random_state=random_state\n",
        "    )\n",
        "\n",
        "    # Split into X and y arrays\n",
        "    x_sample, y_sample = split_x_y(df_sample)\n",
        "\n",
        "    return x_sample, y_sample\n",
        "\n",
        "def test_inference_time_bcnn(\n",
        "    model: tf.keras.Model,\n",
        "    x_test_data: np.ndarray,\n",
        "    num_monte_carlo: int\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Measure average inference time per test sample for a Bayesian CNN using Monte Carlo sampling.\n",
        "\n",
        "    Parameters:\n",
        "        model            : Trained BCNN model\n",
        "        x_test_data      : Test feature matrix [n_samples, n_features]\n",
        "        num_monte_carlo  : Number of Monte Carlo forward passes per sample\n",
        "\n",
        "    Returns:\n",
        "        avg_inference_time_ms : Average inference time per sample in milliseconds\n",
        "    \"\"\"\n",
        "    times = []\n",
        "\n",
        "    # Warm-up forward pass to avoid first-run latency\n",
        "    _ = model(x_test_data[:1], training=True)\n",
        "\n",
        "    for x in x_test_data:\n",
        "        x_input = np.expand_dims(x, axis=0)\n",
        "\n",
        "        start = time.time()\n",
        "        for _ in range(num_monte_carlo):\n",
        "            _ = model(x_input, training=True).numpy()\n",
        "        end = time.time()\n",
        "\n",
        "        times.append(end - start)\n",
        "\n",
        "    avg_inference_time_ms = np.mean(times) * 1e3\n",
        "    print(f\"Average inference time: {avg_inference_time_ms:.3f} ms/sample\")\n",
        "\n",
        "    return avg_inference_time_ms\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1754487915241
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Create sample set for inference time measurement ===\n",
        "x_inference_time, y_inference_time = get_inference_sample_set(\n",
        "    df_test_variety,\n",
        "    df_test_season,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "# === Compute the average inference time ===\n",
        "bcnn_time_ms = test_inference_time_bcnn(\n",
        "    model=bcnn_trained,\n",
        "    x_test_data=x_inference_time,\n",
        "    num_monte_carlo=NUM_MONTE_CARLO\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1754487926286
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "tf",
      "language": "python",
      "display_name": "Python (tf)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.15",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "kernel_info": {
      "name": "tf"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}